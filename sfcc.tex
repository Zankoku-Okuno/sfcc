\documentclass[11pt]{article}
\usepackage{geometry}
\geometry{a4paper}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{float}
\floatstyle{boxed}
\restylefloat{figure}
\usepackage{subcaption}
\usepackage{bussproofs}
\usepackage{fancyvrb}
\usepackage{xcolor}
\usepackage[T1]{fontenc}

%%% IF draft %%%

\usepackage{setspace}
  \doublespacing
  \raggedright
\usepackage{parskip}
  \setlength{\parindent}{2em}
\usepackage{etoolbox}
  \AtBeginEnvironment{verbatim}{\doublespacing}
  \AtBeginEnvironment{tabular}{\doublespacing}
\usepackage{footmisc}
  \renewcommand{\footnotelayout}{\doublespacing}
\newcommand{\maybePage}{\newpage}

%%% ELSE %%%

%\newcommand{\maybePage}{\null}

%%% END %%%

\newcommand{\highlight}[1]{\colorbox{black!30!white}{#1}}

\newcommand{\type}{\star}
\newcommand{\isType}[2]{#1 \vdash #2\;\textbf{type}}
\newcommand{\hasKind}[3]{#1 \vdash #2:#3}
\newcommand{\hasType}[4]{#1 #2 \vdash #3 : #4}

\newcommand{\abs}[3]{\lambda (#1 :: #2)\;#3}
\newcommand{\Abs}[2]{\Lambda #1. #2}

\newcommand{\tuple}[1]{\langle{}{#1}\rangle{}}


\newcommand{\Cue}{\textbf{Cue}}
\newcommand{\Subcont}{\textbf{Subcont}}

\newcommand{\newhandler}{\textsf{newCue}}
\newcommand{\handle}[3]{\textsf{handle}[#1]\;#2\;\textsf{in}\;#3}
\newcommand{\abort}[2]{\textsf{abort}[#1]\;#2}
\newcommand{\capture}[2]{\textsf{capture}[#1]\;#2}
\newcommand{\pushSubcont}[2]{\textsf{restore}\;#1\;#2}
\newcommand{\onWind}[2]{\textsf{onWind}\;#1;\;#2}
\newcommand{\onUnwind}[2]{\textsf{onUnwind}\;#1;\;#2}
\newcommand{\onAbort}[2]{\textsf{onAbort}\;#1;\;#2}


\newcommand\x{\lambda x}
\newcommand{\letin}[2]{\textsf{let }#1\textsf{ in }#2}
\newcommand{\tryfin}[2]{\texttt{try}\;#1\;\texttt{finally}\;#2}
\newcommand{\withCont}[2]{\textsf{withCont}[#1]\;#2}
\newcommand{\onExit}[2]{\textsf{onExit}\;#1;\;#2}
\newcommand{\dynamicWind}[3]{\texttt{dynamicWind}\;#1\;#2\;#3}
\newcommand\lseq{\mathbin\textrm{\guillemotleft}}

\newcommand\F{\mathcal{F}}
\newcommand\A{\mathcal{A}}
\newcommand\pmFpm{^\pm\F^\pm}

\newcommand{\angles}[1]{\langle#1\rangle}

\title{On secure first-class control}
\author{Eric Demko}
%\date{} % delete this line to display the current date

%%% BEGIN DOCUMENT
\begin{document}

\maketitle\maybePage
\tableofcontents\newpage

\begin{abstract}
First-class control (FCC) is a generic name for a variety of techniques for dynamically manipulating program control flow.
The general claim is that FCC allows any control structure to be efficiently implemented within an existing language.
While there has been considerable research into how the different FCC techniques relate to each other, how these techniques relate to software architecture is less well-developed.
As such, FCC is commonly viewed as complex and error-prone: only a few languages incorporate it.
Our claim is that native FCC can be integrated with existing control techniques, and that doing so reduces the likelihood of programmer error.
%The main purpose of this paper is to develop a formal semantics with which language designers can reasonably integrate first-class control into production programming languages.


%%% This was a first attempt at an abstract %%%
%First-class control (FCC) offers techniques for efficient and clear implementation.
%It takes its power from the ability to effect arbitrary re-arrangements of code execution order.
%%On the other hand, the deterministic management of scarce resources is a prerequisite for secure programming.
%%As resources leak, opportunities are opened for denial-of-service attacks, and chained exploits.
%%First-class control has not yet been integrated into major production programming systems, perhaps because it is not known how the operators interact with resource management.
%This re-arrangement can easily lead to developer confusion and therefore also the introduction of serious programming flaws.
%The goal of this paper is find a formulation of FCC that significantly reduces the likelihood of programmer error without sacrificing its expressive power.
%Ideally, this would pave the way for major production programming systems to incorporate FCC and thereby increase their expressivity without sacrificing security.

In this paper, we develop the semantics of an extended lambda calculus incorporating secure, expressive and convenient first-class control.
Our formulation unifies continuations with other non-local control constructs, esp. including exceptions and transactions.
We demonstrate the utility of our system with examples of advanced constructs that may be efficiently implemented by users.
Ideally, this would pave the way for major production programming systems to incorporate first-class control and thereby increase at once their expressivity and security.
\end{abstract}

\maybePage
\section{Introduction}

Exceptions and first-class control (FCC) are deeply interconnected, but are perceived as having wildly different efficacy.
Though exceptions are widely implemented and used, FCC is viewed as dangerously complex -- a tool only for gurus -- and therefore shunned by production programming systems.
Nevertheless, exceptions admittedly come with their own dangers.
Careless use of exceptions renders systems vulnerable to slowdowns, crashes, and data corruption.
These problems manifest themselves because of poor maintenance of resources such as file handles, network connections, locks, and even simple mutable state.
To mitigate this, programmers have developed a notion of ``exception safety''---a usually informal analysis meant to reveal exception-related programming errors.
It is reasonably easy to check for exception safety, even without compiler support, so many developers have judged the advantages of exceptions to outweigh the risks.
Given the deep connection between exceptions and FCC, we can reasonably expect the perceived danger of FCC to be only a product of formulation.

The main contribution of this paper is a dynamic semantics which:
\begin{itemize}
\item incorporates both higher order first-class control along with non-local jump protection, which
\item easily simulates all major forms of non-local control flow (including exceptions and various delimited control operators),
\item is no more likely to cause crashes or mis-manage resources than existing exception systems, and
\item offers a route to more expressive languages in that they would be capable of encapsulating arbitrarily complex control flow structures behind simple interfaces.
\end{itemize}
%The main contribution of this paper is a unification of exceptions, continuations, and several other control structures, thereby offering first-class control more powerful than exception systems, while also no more dangerous.
In the process, we review the non-local control flow techniques already implemented in production programming languages
We also use our system to implement several constructs and illustrate several applications of our system of FCC.
We further point to several additional variations, laying the foundation for discussion to determine the ideal primitives for FCC.

\maybePage
\section{Overview}

\subsection{Fundamental Concepts}
When we---as humans---evaluate a complex expression, we engage in a two-fold process: 1) focus on and evaluate a particular subexpression, and 2) remember that once we have suitably simplified the subexpression, we need to substitute the result back into to our original expression in place of the subexpression.
As we iterate this process, we build up a stack of \emph{contexts} through which will need unwind a value.
A contiguous section of the context stack is called a \emph{continuation}, and the stack as a whole is often simply referred to as \emph{the continuation}.
First-class control (FCC) simply means being able to access continuations and use them as first-class values.
This definition is of course somewhat informal, but a more rigorous definition is not widely available, perhaps due to the breadth of variations and relative rarity of its implementation.

Continuations are a rich source of concepts and terminology, so we will now address the terminology relevant in this thesis.

We say evaluation occurs within the \emph{dynamic extent} of an expression $e$ when we have a context of derived from $e$ onto the stack.
When the evaluator places a context on the stack in response to $e$, we say we have \emph{entered its dynamic context}.
Similarly, when that context is removed from the context, we say evaluation has \emph{exited its dynamic extent}.

Altering the continuation only in the topmost context of the stack is the essence of \emph{local control flow}, whereas \emph{non-local control flow} involves altering the stack by adding or removing whole continuations at a time.

Beyond local control flow, FCC involves at most a few operations on the continuation.

A top-most section of the stack can be selected and saved elsewhere in memory, which we call \emph{capture}.
This stack section is then \emph{reified}---made available as a value.
Often a captured continuation is also removed from the stack, which is called an \emph{abort}, although it is not necessary for these to occur together.
An operator which performs any combination of capture, abort and reification is called a \emph{control operator}.
Any reified continuation may later be \emph{restored}: copied back onto the stack on top of the current continuation.
Continuations may even even be restored multiple times.
In contrast, exceptions remove a section of the control stack, but do not reify it and therefore cannot restore it\footnote{Some exception mechanisms may capture the stack behind the scenes for use in generating stack traces, but do not allow programmer access to the continuation itself. Generating a stack trace is a distinct operation from continuation capture.}.

When a control operator affects a proper subsection of the stack, we say that it is \emph{delimited}, as opposed to \emph{undelimited}, or operating on the entire stack.
The section of stack operated by a delimited control operator is determined using special contexts called \emph{delimiters}, \emph{prompts} or \emph{stack marks}.
Operators that place a delimiter on the stack are called \emph{delimiting operators}.

%TODO:
%Without control operators, the context strictly obeys a stack discipline, but once we are allowed to capture portions of the stack as values, we can then apply the full power of lambda calculus to control the evaluator's dynamic extent at runtime (i.e. possibly in response to input).

\maybePage
\subsection{Motivation}

While the above definition is accessible to implementors, it leaves the advantages of first-class control opaque to the programmer.
The advantage of FCC is that \emph{it allows for the abstraction and reuse of any pattern of control flow}.
That is, any control flow structure, no matter how complex, can always be centralized to reduce maintenance costs.
First-class control is thus a natural extension of existing high-level languages, which have already brought the power of abstraction to patterns of algorithm, data, module, and syntax.

As a simple example, a common pattern is an accumulation loop with early-exit.
We can use FCC to efficiently implement this pattern and reuse it without difficulty, as in Figure~\ref{fig:ex-foldl/ee}.
We will give further analysis in \S\ref{subsec:earlyExit} after we have developed our system of first-class control.

\begin{figure}[H]
\caption{Define the accumulation loop with early exit}
\label{fig:ex-foldl/ee}
\begin{verbatim}
(define (foldl/ee zero xs body)
   (define mark newCue)
   (define (break val) (abort mark val))
   (handle mark id
      (foldl zero xs (body break))))

(define (all? xs p)
   (foldl/ee true xs (lambda (break acc x)
      (if (p x)
          true
          (break false)))))
\end{verbatim}
\end{figure}

Expressivity---the reduction of manual pattern-based programming---is not however a goal in itself.
Rather, expressive power should be used to increase the quality of the software, both in terms of efficiency and reliability.
Native first-class control enables just such an improvement.
Consider: library writers often create interfaces---esp. for working with external resources---that require the exposed procedures to be used in rigidly constrained orders.
With first-class control, library writers can codify these patterns in executable code rather than mere documentation.

\begin{figure}[H]
\caption{File API without FCC}
\label{fig:fileAPInoFCC}
\begin{verbatim}
(define (open filepath mode) ...) ;; returns a file handle

;; once you're done with the file, call this or the resource will leak
(define (close handle) ...)

;; don't call read or write on a file that's been closed,
;; or Bad Things(TM) will happen!
(define (read handle) ...)
(define (write handle bytes) ...)
\end{verbatim}
\end{figure}

As a simple example, consider the hypothetical API for working with files given in Figure~\ref{fig:fileAPInoFCC}.
Improper ordering to any of these four procedures at runtime can cause a supposedly reliable system to fail.
Although the requirements on the structure of control-flow are documented, they are not enforced by the language.
Thus, if the programmer has incomplete knowledge of the documentation or of his program, it is likely that one of these requirements will be violated.

\begin{figure}[H]
\caption{File API with FCC}
\label{fig:fileAPIFCC}
\begin{verbatim}
(define (withFile filepath mode body) ... (body the_handle) ...)
(define (read handle) ...)
(define (write handle bytes) ...)
\end{verbatim}
\end{figure}

With FCC, we can present a different interface by combining the effects of the \texttt{open} and \texttt{close} procedures into a single higher-order procedure \texttt{withFile}.
The \texttt{withFile} procedure opens the file, exposes it only to the \texttt{body} procedure, and ensures that the file will be closed immediately after the body is finished.
Such an interface is not only smaller, and therefore easier to comprehend, but also guarantees that the file handle is used in accordance with the control-flow structure requirements.
Simply by interface design, we can ensure\footnote{For reasons of space, we have not shown how to ensure that a file handle does not escape from the body of \texttt{withFile}. A clever, well-known technique takes advantage of a type checker to do this. \cite{StateInHaskell}} that read/write cannot be performed on a closed file and file handles are closed quickly.
In this way, any possibility of the aforementioned programmer errors is eliminated.
Even when control-flow dependencies become complex, FCC is still able to encapsulate the complexity, providing APIs that are resilient even in the face of incomplete programmer knowledge.
Without first-class control, such guarantees are generally impossible to obtain practically.


There are downsides to first-class control. After all, ``an increase in expressive power comes at the expense of less `intuitive' semantic equivalence relations.''\cite{Felleisen90expressive}
For FCC in particular, we lose the ability to easily map from the static source code to the dynamic control flow of a program.
The implications for program reliability and security are poorly-understood, and are assumed to be significant.
%FIXME rephrase, whatever that means
On the one hand, this means that managers of sensitive production systems should be skeptical of FCC, but on the other, it means that no-one is capable of systematically evaluating the relative advantage of incorporating it.

It is relatively easy for the designer of a new language to include whatever system of FCC they see as most advantageous.
Implementing a desired system of FCC in an existing language can be rather more difficult.
Nevertheless, it doesn't make much difference from an academic viewpoint whether FCC is implemented natively or not.
There are two broad alternatives to native first-class control.

First, a programmer may fall back on pattern-based programming.
This is an insidious alternative, being both very easy to accomplish when first writing the software, but very difficult to untangle and maintain later.
In general, many properties of pattern-based programs cannot be verified with anything short of whole-program analysis.
Furthermore, the structure and intent of such a system are entirely opaque from the source code.\cite{Felleisen90expressive}
Both of these drawbacks significantly raise the both the cost of maintenance and the risk of damage due to the program.
We shall see concrete examples of this in our review.

Second, it is possible to simulate first-class control with varying ease depending on the host language.
It is our opinion---after Greenspun's Tenth Rule---that these implementations will, with overwhelming probability, be incomplete, ad-hoc, buggy and/or slow.
In particular, it is difficult to arrange that simulated FCC will not interfere with existing control operators.\cite{addDelimControlProduction}
Furthermore, simulation may increase the expected size-complexity of the simulated operators.\cite{finalShiftForCallcc}
In this case as well, the drawbacks are not~likely worth the costs and risks in a production system.
%Examples of simulated FCC already exist in the literature (CITEME I know of the javascript one at least), but nevertheless fail in one or more aspects.



It is clear then, that if we are to maximize the reliability and security of computer systems without sacrificing efficiency, we must develop a system of first-class control wherein programmers may easily develop intuitions about program equivalence.
This is exactly the task we solve in this paper.
We will not argue that we have the unique best possible solution, but our system is clearly the beginning of the end for the confusion that has previously surrounded first-class control in production languages.

\maybePage
\subsection{Our System}

In our thesis, we develop $\lambda_{SFCC}$, a lambda calculus augmented with operators for both first-class control and the secure handling of resources.
The key complaint we address is that there are too many variations of FCC systems, and the relationships between them are only defined pairwise.
Further, these systems vary in their capabilities; most systems of FCC in the literature are unable to intercept non-local control flow.

Our system enjoys various advantages, depending on the systems it is compared to.
The ultimate benefit of $\lambda_{SFCC}$ is that it provides for deterministic resource initialization and cleanup, even in the presence of arbitrarily complex non-local control flow.

Formal models of first-class control in the literature are unconcerned with external resources, and so offer no operators for their management.
By contrast, $\lambda_{SFCC}$ not only provides non-local control flow operators, but also incorporates operators to intercept non-local control flow.
The interception of control flow in the plain lambda calculus is so simple as to be taken for granted, so it is unsurprising that this capability was overlooked in the literature.

Most production languages do not offer any form of FCC.
The advantage of $\lambda_{SFCC}$ is, of course, that FCC is possible, and also that it is reasonably safe to use.
Almost all languages which offer exception handling, a limited form of non-local control, do not formalize their exception handling mechanisms.
Since $\lambda_{SFCC}$ is capable of implementing various exception handling mechanisms, we immediately gain a formal specification of these control structures.

There are a few production languages which do offer a resource management operator, \texttt{dynamic-wind}, alongside FCC, though these languages are relatively little-used.
Our advantage against these is that we have a formal specification\footnote{Though there is a formalization of Scheme's \texttt{dynamic-wind} in \cite{SchemeOpSem}, it is \textit{post-hoc}, not part of the language standards.}, and our system is general enough to easily implement their breadth of operators, and more besides.

Thanks to our system's similarities to \cite{Gunter:1995} and \cite{MFDC}, we expect a practical type system to be able to rule out several classes of error related to the use of FCC.
Note that to achieve practicality, type systems of this sort must make a small compromise in their soundness: this is the same compromise that exists in unchecked exception systems.
We do not believe this to be a cause for concern, since unchecked exceptions are already widely used in production.
Nevertheless, alternate type systems are certainly possible, each making different trade-offs regarding soundness vs. ease of use.
In this paper, we focus only on the dynamic semantics of FCC, leaving a static semantics to further research and the preferences of language designers.

\maybePage
\subsection{Structure of this Paper}

In Part One, we examine a number of control flow constructs that already exist in production programming systems, first looking at exception handling, then at existing first-class control operators.
We consider resource-acquisition-is-initialization in \S\ref{RAII} and try-catch-finally in \S\ref{try-finally}.
We also look at some less-widely implemented structures.
Statements in Python, C\# and Java share properties detailed in \S\ref{context statements}.
Go's defer-expressions and D's scope statements are described in \S\ref{defer}.
Monads are examined in \S\ref{subsec:exceptionMonads} specifically in the context of Haskell, though they can also be used in other statically-typed functional languages.
In \S\ref{sec:FCC} we turn to first-class control, beginning with a small tutorial.
In \S\ref{undelimControl} and \S\ref{delimControl} we examine a variety of first-class control operators.
In \S\ref{dynamic-wind} we explain Scheme's dynamic-wind construct for managing resources under FCC.
A broad analysis of these constructs' capabilities, including advantages and disadvantages, is given in \S\ref{sec:review}.

In Part Two, we apply the lessons learned in Part One to design $\lambda_{SFCC}$, a novel system of first-class control which is also capable of managing external resources with the same ease as in existing languages.
We describe the system in \S\ref{sec:SFCCcalculus};
the whole system is fairly large, but can be split into a few small components, each of which we detail in a subsection there, along with the system as a whole in \S\ref{subsec:summarySFCCcalculus}.
Then, in \S\ref{sec:relationship}, we analyze $\lambda_{SFCC}$ in relation to its influences.
We also examine its relationship to exception handling (\S\ref{subsec:implExn}) and a few other important FCC systems (\S\ref{subsec:implMisc}).
In \S\ref{sec:efficientImpl}, we give an alternate formulation of the calculus in terms of a transition system, which can be directly translated into an efficient interpreter (constant overheads aside).
Finally, we examine a wide range of applications of FCC in \S\ref{sec:applications}, underscoring the effectiveness of FCC as an implementation technique.



%\section{SOMEWHERE}
%
%
%From HDBC docs: ``Bad Things (TM) could happen if you call this [disconnect function] while you have Statements active. In more precise language, the results in such situations are undefined and vary by database. So don't do it.''
%
%The security of a system depends on its developers, even though they generally are unable to grasp the subtleties of the entire system at once.
%Therefore, we wish to make the secure solution also the path of least resistance.
%%In our review, we will also highlight aspects of language features that cannot be easily abstracted.
%% After all, at 4:58 on a Friday you've just gotten it to work in the success case and are ready to go home, so you commit your code. What are the chances that you remember Monday to review exception safety on all that hairy work that so frustrated you friday? What are the chances QA will discover the rare and subtle non-local control flow bug? Does your organization have a QA department? Development practices are imperfect, and even if they were perfect, they would still inevitably be violated at some point in the development of any significant system. Making the path of least resistance closely follow the most secure path will provide developtime defense-in-depth and lead to more robust systems.
%
%
%
%
%Library authors often document their API with terse, informal specifications of the correct patterns of use of their functions. (HDBC example above, numerous cases in openGL)
%A user of the library may not read that piece of documentation, remember it, understand it, or be able to correctly apply it in their own complex code; in such cases, the reliability of the user's program may be comprimized.
%When the author of a library can specify their own control structures, there is no need for these restrictions.
%That is, the library itself can guarantee correct patterns of use regardless of the user.
%FCC offers exactly that capability to library writers, simplifying use of the library and enabling client code to be more reliable more easily.

\maybePage
\part{Literature Review}

If we are to unify existing systems of control flow, we must examine a wide range of existing control structures. To maintain our focus on reliability, we restrict our examination to those control features available in production languages. First, we will focus on non-local control flow without continuation capture: exceptions and related concepts. Then, we will examine first-class control as it exists in Lisp dialects, to date the only production languages which support a variety of first-class control operators. Our goal is to identify successful patterns of control flow for use both as inspiration for our control calculus, and as a sanity check to ensure that the control calculus developed is expressive enough for general use.

\maybePage
\section{Exception Handling}

\subsection{RAII}
\label{RAII}

One of the very first methods developed for automatic resource cleanup is called ``resource acquisition is initialization'' or RAII.
The technique fundamentally relies on method overriding and so appears only in object-oriented languages.
It ties an object's initialization and deinitialization to its lifetime using a constructor(s) and destructor.

In C++, when an automatic variable becomes unavailable, such as when it goes out of scope or its stack frame is removed, its destructor (if any) is implicitly called.
RAII takes advantage of this feature by placing any relevant cleanup code into the destructor.\cite{cplusplusLanguage}
Since the lifetime of automatic variables is easily apparent, RAII performs deterministic cleanup.
However, C++ cannot detect lifetimes of heap-allocated data, and so for these objects, destruction must be performed manually.
Manual memory management is not only expensive to develop, but also contributes to serious bugs which can easily compromise secure systems.\cite{WeimerNecula08}


In Java, C\#, and similar languages which use garbage collection, destructors\footnote{Some languages call these ``finalizers'', but the result is essentially the same.} are called when an object is collected.
Unfortunately, the time to detect, and therefore clean up unreachable objects is non-deterministic.\cite{JavaStandard}\cite{cSharpStandard}
Whenever the resource is not in physically infinite supply, the garbage collector will race the program to maintain the illusion of automatic cleanup.\cite{WeimerNecula08}
Further, the order in which objects are finalized during garbage collection cannot be predicted, so the technique is unsuitable when there are ordering dependencies between several objects' finalizers.\cite{WeimerNecula08}
%Depending on the implementation, the teardown code may never be executed, such as when a program terminates without having collected all its allocated memory.
This should not be surprising, since garbage collectors are meant mainly to free up memory during program execution rather than as a means of registering callbacks.
In short, the use of finalizers tied to the the garbage collection cycle is not a generally suitable technique for timely and correct resource management.

RAII succeeds well when faced with managing external resources such as database connections, but when faced with internal logical state management, it offers no advantage.
For example, in languages with garbage collection, the lack of determinism can mean that RAII is even technically unsuitable.
Beyond the technical limitations of RAII, there is also a psychological one.
Every definition of cleanup code needs its own class definition, which is quite large considering the few lines that are really important.
With these pressures, programmers will, with good reason, shun RAII and resort to ad-hoc techniques that are likely to introduce other flaws.


\maybePage
\subsection{Try-finally}
\label{try-finally}

Try-finally is a fairly simple idea: whenever and however control exits from the \texttt{try} block, the code in the \texttt{finally} block will be executed.
The ubiquity of try-finally can be traced to several advantages.
In this approach, cleanup is deterministic.
Try-finally and throw constructs are also easy to detect and reason about both programmatically and visually, making auditing for exception safety straightforward.
Finally, control flow during exception handling is particularly transparent compared to other non-local control techniques.

Unfortunately, the transparency of exception handling is largely due to the fact that exception handling code cannot be abstracted;
it is transparent in the sense that it is always exposed.
Mainstream OO languages effectively force developers to resort to pattern-based programming, writing out common exception handling code for every use.
As mentioned before, pattern-based programming is costly: it requires developers to take time writing more code, and opens the code to regressions during maintenance.

Consider the code pattern in Figure~\ref{tryPattern}, for example.
In the pattern, \textit{A} is to be replaced by some business logic and \textit{B} by any additional cleanup.
If any code within the \textit{B} slot could throw an exception, then \texttt{connection} will not be closed as intended.
If, for example, some logging were to be added in \textit{B}, an I/O error could occur -- in a distributed system, a simple network outage is sufficient -- causing an additional exception before the rest of the cleanup and preventing the connection from being closed.
This example is very simple not only for reasons of space, but also because it is at the core of the problem.
For a thorough examination of examples found in the wild, we refer the interested reader to \cite{WeimerNecula08}.

\begin{figure}[H]
\caption{Try-finally Pattern}
\label{tryPattern}
\begin{verbatim}
try {  
    var db_connection = new DBConnection(...);

    A...
}
finally {
    B...

    if (db_connection != null)
        db_connection.close();
}
\end{verbatim}
\end{figure}
%        EmailerClass.EmailMe("ret = " + ret.ToString() + "!!!");  

Different programming languages have very different semantics in the event an exception attempts to propagate beyond a catch or finally block.
Many languages, including Java and Python, allow the new exception to propagate, sometimes with additional information in the stack trace.
Perhaps the most reliable solution is to instead terminate the program, as in C++.\cite{evolutionCplusplus}
This decision reflects the notion that an exception handler must be exception-safe.
That is, an exception handler should only do trivial cleanup tasks, not respond with exception-prone actions.
Guaranteeing this level of exception-safety is the first of two major solutions to mitigate the problems of try-finally.

Exception safety may be automatically determined by the compiler using checked exceptions.
However, checked exceptions have largely been rejected in production designs.
Although Java does have a checked exception system, safety is only checked at the method level, not as relates to the example here.
Further, the system co-exists with the more commonly used unchecked exception system.
Support for checked exceptions is lacking largely because checked exceptions in Java have evidenced their uselessness and inconvenience in common languages (such as Java, C++, and C\#), and of course because they are quite impossible in dynamic languages.

The second solution, in a language with good support for first-class functions\footnote{By good support, I mean that the language should make it easy to use functions in a first-class manner. In particular, the syntax should not be overly verbose, static scoping should be default, and tail call optimization should be performed. Many common languages, such as Java, C++, Javascript and Python do not meet these requirements even though they technically allow first-class functions.}, to the problems of Example~\ref{tryPattern} is to use continuation-passing style (CPS).
The best example of this technique is in Ruby, where it is unobtrusive to pass a single continuation during function call.
For example, Ruby users can simply write \texttt{File.open("example.txt", "r") do |fp| ... end}, where cleanup is centralized in the \texttt{.open} method, and the user can supply their own file manipulation code in the block.\cite{programmingRuby}

Unfortunately, most languages are not able to do this so cleanly, either because they have not (yet) adopted first-class functions, or because the CPS transform must be manually performed.
Indeed, even in Ruby only a single continuation can be passed naturally; as soon as a second is needed, manual transformation is required.
Even then, when the continuation needs to be passed to additional functions, the user must revert to the tedious and error-prone continuation-passing style.
We have yet not mentioned lazy languages, but it turns out that these are unsuitable as well.
Although they can fully overcome the syntactic issues, lazy languages erode determinism, which is the key benefit of try-finally.
%Additional techniques can restore the determinism, but we will return to this issue in Section~\ref{subsec:exceptionMonads}.

The combination of unchecked exceptions and the inability to abstract is detrimental to reliable programming.
Careful code review may be able to mitigate these exception-safety bugs, but from both accuracy and cost-efficiency standpoints, it would be much better to have compiler support.


\maybePage
\subsection{Context Statements}
\label{context statements}

The development of the ``with statement'' in Python, the ``using statement'' in C\#, and the ``try-with-resources statement'' in Java bear a remarkable resemblance to the development of the for-each loop.
The for loop was so commonly used as \texttt{for (x = start(xs); notAtEnd(x); x = getNext(x)) \{...\}} that language designers decided to codify this pattern as the syntax \texttt{foreach x in xs \{...\}}.
Similarly, the pattern in Figure~\ref{tryPattern} is so common that some language designers have chosen to codify the pattern roughly as \texttt{with x = some\_resource \{...\}}.
Although Python, C\# and Java each use different names and syntaxes for largely identical concepts, for simplicity we will simply refer to these constructs as ``context statements''.

The key ingredients of a context statement are an expression and a block.
The statement may also bind the result of evaluating the expression to a variable for use inside the block.
Further, the result of the expression is meant to be equipped with methods for setup and teardown code.
Before the block is entered, the setup code is executed, and just before the block is exited, by any means, the cleanup code is executed.\cite{JavaStandard}\cite{PEP343}\cite{cSharpStandard}



%(ohh, hey, since they are deterministic, we can use them for dynamic binding: makes a good example)
%parameterize(name, val) returns a FluidCxt with appropriate exit code
%there's a global db of fluidvars
% a function fluidVar(name) gets the current value

%\begin{verbatim}
%_fluidVars = dict()
%_sentinal = object()
%
%def getFluid(name):
%    return _fluidVars[name]
%
%class parameterize:
%    def __init__(self, name, value):
%        self.name, value = name, value
%    
%    def __enter__(self):
%        if self.name in _fluidVars:
%            self.old = _fluidVars[self.name]
%        else:
%            self.old = _sentinal
%        _fluidVars[self.name] = value
%
%    def __exit__(self, *args):
%        if self.old is _sentinal:
%            del _fluidVars[self.name]
%        else:
%            _fluidVars[self.name] = self.old
%\end{verbatim}


%in python, you can at least define a context manager in a functino rather than a class; very similar to RAII, but adds determinism in gc'd languages


%Perhaps in response to the problems illuminated by Figure~\ref{tryPattern}, some languages have incorporated the design into the language definition. This includes the with statement in Python and the using statement in C\#. These statements include an expression and a block of code. After the expression is evaluated and optionally bound to a name for the scope of the block, some setup code is executed before the block is executed. No matter how control exits from the block, cleanup code is guaranteed to be executed before continuing. Dispatch to both the setup and cleanup code are determined by the result of the expression, and generally involves ad-hoc polymorphism.

Figure~\ref{withStatement} is an example of a with statement in Python (the corresponding C\# and Java versions are only superficially different).
In it, the expression \texttt{open(filePath, 'r')} opens a file in read-mode.
In this case, no additional setup is given, so the resulting file handle is bound to \texttt{fp}.
The body of the statement is then executed.
If no exceptions are raised, then immediately after the block, the cleanup code is called on the file handle; in Python, this is the \texttt{\_\_exit\_\_} method.
If an exception is raised in the body, then the cleanup method is called before propagating the exception.
In the specific case of file handles, the exit method simply closes the file.

\begin{figure}[H]
\caption{With statement}
\label{withStatement}
\begin{verbatim}
with open(filePath, 'r') as fp:
    do_something(fp)
\end{verbatim}
\end{figure}

Figure~\ref{withStatement} maps neatly to the pattern of Figure~\ref{tryPattern}.
As such it retains advantages of the more general exception-handling approach, in particular determinism.
It saves only a very little in source code size, but importantly, its use does not offer any ability to compromise the exception safety of the handler.
Because the try-finally pattern is so ubiquitous, preferring context statements eliminates many opportunities for error.
In this author's experience, the benefits for program maintenance outweigh the cost of a slightly larger language.
However, there are inherent limitations with strategy of na\"ively codifying patterns into syntax as has been done with context statements and for-each loops.

\begin{figure}[H]
\caption{Accumulation loop}
\label{accumLoop}
\begin{verbatim}
acc = start_value
foreach x in xs:
    acc.add_element(x)
\end{verbatim}
\end{figure}

Consider for the moment the standard accumulation pattern give in Figure~\ref{accumLoop}.
This is a very common pattern, but it has not been codified into the syntax of any language, even though there are multiple locations where errors can be introduced.
For example, the loop body may contain an early exit, a conditional path may accidentally omit an addition to the accumulator, and so forth.
In contrast, consider the higher-order functions \texttt{map} and \texttt{fold}.
The \texttt{map} function corresponds to for-each, and \texttt{fold} solves accumulation problems, but importantly, \emph{they are not built into the language}.
As such, higher-order functions are able to codify many looping constructs without extending the language.

Similarly, context managers solve one particular problem very well, but cannot be generalized.
Context managers are unable to encode patterns of stopping error propagation, logging errors, and so forth.
No matter how many specialized control constructs we add to the language, there will always be patterns that have not been codified; such a language is fundamentally limited in its abstractive power.
Perhaps the most accessible expression of this idea is that ``it is impossible to anticipate all possible needs for control abstractions during the design of a programming language.''\cite{ControlDelimitersHierarchy}

Finally, we mention some contact with the RAII approach.
In particular, both solutions externalize the setup-cleanup code pairing to another class.
One may think the strategies share the problems of one-off cleanup logic, but it turns out that for single-use scenarios we can instead use try-finally.
Switching to try-finally does not increase the likelihood of programmer error, because in either case there is one definition of the logic: in-line with try-finally, versus in an external class with context managers.
It is only when a definition is duplicated that the possibility for error increases.


\maybePage
\subsection{Defer Statements}
\label{defer}

The languages D and Go share a similar language feature whereby expressions may be registered to be evaluated before the call stack unwinds \cite{DReference}\cite{GoLanguage}. We will call these statements ``defer statements'' after Go terminology\footnote{D calls them ``scope guard statements''.}. This particular feature is quite new, so there are several differences between the two languages, though they share the same idea.

In the case of Go, each stack frame maintains an additional stack of unevaluated expressions. When control reaches a defer statement, $\texttt{defer}\;e$, the expression \textit{e} is pushed to the stack, but not evaluated. When the stack frame is popped (whether from normal return or due to an exception), the expressions it had built up are evaluated in the reverse order they were pushed.

\begin{figure}[H]
\caption{Go-style defer statement}
\label{goDeferStatement}
\begin{verbatim}
fp, err := os.Open(filepath)
if err != nil {
    return
}
defer fp.Close()
do_something(fp)
\end{verbatim}
\end{figure}

A simple defer statement in D is written $\texttt{scope(exit)}\;e$.
D can also discriminate between normal and abnormal exits with $\texttt{scope(success)}\;e$ and $\texttt{scope(failure)}\;e$ respectively.
As in Go, the expression is unevaluated when control passes over the statement.
When control exits from a scope block, all the \texttt{scope(exit)} statements are executed, as well as all the \texttt{scope(success)} statements if the block was exited normally, or all the \texttt{scope(failure)} statements, if the block was exited because of an exception.
This allows D's defer statements are registered at compiletime, rather than at runtime as in Go.
Nonetheless, the effect is very similar, as the registered expressions will be evaluated in reverse order.

The ability for D to discriminate between normal and abnormal exit is very useful for performing atomic operations.
For example, Figure~\ref{Dscope} shows a program which performs an operation on a file and logs the result.
When multiple error-prone operations must be performed together atomically, this technique can save significant development time compared to try-finally or RAII\cite{DExceptionSafety} (which are also available in D).

\begin{figure}[H]
\caption{D-style defer statement}
\label{Dscope}

\begin{verbatim}
void processFile(string filename) {
    auto fp = File(filepath, "r");
    scope(success) logger.info("Success.\n");
    scope(failure) logger.info("FAILURE:" + filepath + "\n");
    scope(exit) logger.info("Attempting file... ");
    do_something(fp);
}
\end{verbatim}

\end{figure}

Unfortunately, the defer statements of both languages share with try-finally the same the inability to be abstracted.
Although patterns of defer statements are generally smaller than patterns of try-finally, they are nevertheless error-prone and cannot be recommended as a general solution for secure software.


%Finally, we would like to note one generalization of defer statements.
%Weimar and Necula present the concept of compensation stacks as an intriguing solution to resource management problems.
%Although it was developed from the notion of a ``linear saga'', it may perhaps best be described here as a generalization of Go's defer statement construct where the stack of deferred expressions is first class, including facility for the programmer to construct additional stacks at runtime.
%However, the authors admit some weaknesses of the approach, and has not undergone thorough testing in production systems, so we will not treat it deeper.


\maybePage
\subsection{Purity and Monads}
\label{subsec:exceptionMonads}
The lazy, pure functional programming language Haskell takes an interesting approach to exceptions in idiomatic code.
Although Haskell incorporates unchecked exceptions as a language feature, native exceptions and exception handling are not prevalent in Haskell code.

Although Haskell is a pure functional programming language, is is meant for use in real-world systems.
To this end, Haskell adopted an idea from category theory called ``monads.''
A detailed discussion of monads in Haskell and their applications is beyond the scope of this paper, but we point the interested reader to \cite{awkwardSquad}.
For our purposes, we can simply say that monads are able to use the type system to make and track very fine-grained distinctions concerning computational effects.
Furthermore, monad values can be easily composed, so large, effectful computations, called ``actions,'' can be easily built from smaller actions, the same way we might build a large procedure by calling several smaller procedures in an imperative language.
%Finally, because Haskell is lazy, functions over monads are able to abstract any pattern of action composition.

In Haskell, the choice was made early that any primitives that affect or depend on the external environment produce values in the IO monad.
For example, the action of reading a byte from a file is a value of type \texttt{IO Word8}; the byte cannot be taken out of the enclosing monad, but values in the IO monad can be flexibly combined together.
In this way, Haskell retains the ability to perform side-effects as in imperative languages, but everywhere a side-effect might occur is noted in the (strong) type system.
Since exceptions are impure, any function that might cause an exception (directly or indirectly) must be an action in the IO monad.

As other authors have noted \cite{ReflectionWithoutRemorse}(FIXME I know I have more direct citations for this), the advantage of monads in Haskell is that we can be guaranteed that only a narrow range of effects can occur within a monadic computation.
Unfortunately, the number of primitives available in the IO monad in particular is so broad that its presence is not informative in this way.
Although a value may be in the IO monad only so that it can take advantage of exceptions, we cannot deduce from the type alone that the value will not, for example, delete files from disk.
Instead, new monads are regularly implemented which give stronger guarantees as to their range of possible behavior.

One such monad is the \emph{maybe monad}, on which we will heavily focus here, but note that there are many more exception-like monads available, including the general-purpose \emph{error} monad, which is capable of reporting any additional information required.
A value of type \texttt{Maybe a} can either be a unit value, \texttt{Nothing}, or a value of type \texttt{a}, wrapped by the \texttt{Just} constructor (e.g. \texttt{Just 5} is of type \texttt{Maybe Int}).
Since the type \texttt{Maybe a} is distinct from the type \texttt{a}, values of one type cannot be used in a context where the other is expected.
Instead, we must use case analysis to ``unwrap'' the underlying \texttt{a} value, but this leads us naturally to consider the case where there is no such value!
Figure~\ref{maybeMonad} shows examples where we define an action (\texttt{divide}), compose it into a larger action (\texttt{calculate}\footnote{The \texttt{calculate} function attempts to compute $\Sigma_{i=^-10}^{10}{1/i}$ with a series of \texttt{divide} actions interspersed with pure additions, which is clearly undefined due to the term $1\over0$.}), and safely unwrap the monad (\texttt{main} or \texttt{main'}), naturally taking into account both the successful and unsuccessful cases.
These patterns are heavily preferred in Haskell over native exceptions.

\begin{figure}[H]
\caption{Maybe monad in action}
\label{maybeMonad}
\begin{verbatim}
divide :: Float -> Float -> Maybe Float
divide _ 0 = Nothing
divide x y = Just (x / y)

calculate :: Maybe Float
calculate = foldM (\acc x -> (acc +) <$> divide 1 x) 0 [-10 .. 10]

main = case calculate of
    Nothing -> putStrLn "Divide by zero error"
    Just result -> print result
main' = fromMaybe (putStrLn "Divide by zero error") print calculate
\end{verbatim}
\end{figure}

The advantages of this technique are clear if we consider Haskell's monads in relation to monads in other languages\footnote{Monads are an abstract concept appearing very commonly, whether it is realized or not. Although elemenatry schoolers can perform addition and multiplication, they are not likely to know that they are operating within an algebraic ring structure. Likewise, every \texttt{if (obj != null) f(obj);} is a pattern-based implementation of a monad.}.
To be concrete, let us consider the null pointer in Java, \texttt{null}.
The null pointer can be used in any context where an object is expected, even though it cannot satisfy any requests for data or any method calls.
Under these circumstances, the compiler cannot guarantee that values stored in a variable of class A can actually be used as a value of class A; all non-primitive-type variables are vulnerable to holding a null pointer instead of an instance of A.
As such, it is not uncommon to obtain a \texttt{NullPointerException} at runtime, which is almost always the result of programmer error rather than system error.
To avoid this possibility, we need to add code checking some objects against \texttt{null}, but the compiler offers no help in identifying where there is danger.

In contrast, Haskell has no need for null pointers, since we can always use the maybe type.
Monads such as maybe allow us to distinguish between an object which really is an object versus an ``object or nothing'', which could be an object or a null, and also to conveniently manipulate both.
Because Haskell is strongly-typed, everywhere a null (a \texttt{Nothing}) might occur can be audited at compile time, and the programmer forced to consider the often neglected edge-case.
The use of monads, in this author's direct experience and in communications with other programmers, has reduced the incidence of bugs at runtime dramatically and has led to significantly more reliable programs.
Indeed, a common phrase in the Haskell community is that ``if it compiles, it works'', which speaks very well of the reliability of Haskell programs, even if it is not strictly true.


The downside of monads is that any control structure they make available is simulated, rather than native.
In particular, while action composition may be efficient enough when right-associated, many monads perform asymptotically worse when left-associated.\cite{ReflectionWithoutRemorse}
For simple exception-like monads, improper association can lead to the exception being needlessly propagated through every following action when we would prefer simply to short-circuit the entire computation at the point where the problem occurred.
In more complex monads, such as those incorporating additional effects, improper association can cause asymptotically worse performance.
Authors of new monads or composition functions must be very careful to consider the performance tradeoffs of their implementation, which can be quite subtle, especially in a lazy language such as Haskell.
Monads certainly shine in their static semantics, but their dynamic semantics shows little concern for their efficiency, especially in light of the ubiquity of monadic constructs.



%\begin{verbatim}
%with :: (Monad m, Resource a) => m r -> (r -> m a) -> m a
%
%with $ openFile "greeting.txt" $ \fp ->
%    hPutStrLn fp "Hello, world!"
%\end{verbatim}


\maybePage
\section{First-class Control}
\label{sec:FCC}

\subsection{Introduction to First-class Control}
\label{subsec:intro-to-fcc}

As we have mentioned, first-class control is not a widely-implemented language feature, and so many readers will correspondingly have little or no experience with or intuition for FCC.
This section gives several short examples of a shift-reset---a particular flavor of FCC---in the programming language Racket, a Scheme dialect.
Readers already familiar with FCC can skip to the next section.
If you are following along in a Racket interpreter, then you will need to import the control operators library with \texttt{(require racket/control)}.

A shift-reset system adds two new special forms to the language, \texttt{(reset \textit{expression})} and \texttt{(shift \textit{identifier expression})}.
In \texttt{shift}, the identifier is bound within the expression, but being able to determine precisely what value is bound is exactly the intuition for FCC we are developing now.

Our first examples below appear to show that \texttt{reset} has no effect on a value passed to it.
We may as well remove the calls to reset from the examples, and everything would work out the same.
When there are no calls to \texttt{shift}, indeed \texttt{reset} has no effect on the process.

\begin{verbatim}
(reset 10)
  => 10
(+ 1 (reset 10))
  => 11
\end{verbatim}

In our next examples, we add \texttt{shift}, first just inside the \texttt{reset}.
Again, there seems to be no effect.
However in the second example, we move the reset out further, so that \texttt{(+ 1 ...)} is inside the \texttt{reset}, but outside the \texttt{shift}.
Here, it seems like the interpreter has lost track of the \texttt{+ 1} call, so we get 10, not 11.
Where did it go?

\begin{verbatim}
(+ 1 (reset (shift k 10)))
  => 11
(reset (+ 1 (shift k 10)))
  => 10
\end{verbatim}

So far, we have not made use of \texttt{shift}'s identifier, so we will do that now.
In the first expression, we see that \texttt{k} acts like a function, and that introducing it mysteriously makes up for the missing \texttt{+ 1} operation.
In the second expression, we see that we can call \texttt{k} multiple times, and that it appears to act like a successor function.
However, the third example shows that \texttt{k} is bound in a very strange way:
by changing the \texttt{+ 1} between \texttt{reset} and \texttt{shift} to \texttt{* 2}, we see that \texttt{k} is then bound to a doubling function.
So, the identifier in \texttt{shift} is bound to a very interesting value, which is something like a function whose definition depends on the program \emph{outside} of the \texttt{shift} form.

\begin{verbatim}
(reset (+ 1 (shift k (k 10))))
  => 11
(reset (+ 1 (shift k (k (k 10)))))
  => 12
(reset (* 2 (shift k (k (k 10)))))
  => 40
\end{verbatim}

The next examples develop systematically.
They consist of a core \texttt{(shift k 100)} expression, which should create some mysterious function-like object, but ignore it.
There are also calls to add one, add ten, and a reset form.
We move the reset form progressively inwards, which has the effect of including more operations in the value of \texttt{k}.
We see then, that the effect of this function-like object is determined not by anything outside the \texttt{shift} form, but only by what is between the \texttt{shift} and \texttt{reset}.
Although we will not show it, it turns out that the value of \texttt{k} depends on what is between the \texttt{shift} that creates it and only the nearest enclosing \texttt{reset}; additional \texttt{reset} forms make no difference.

\begin{verbatim}
(reset (+ 1 (+ 10 (shift k 100))))
  => 100
(+ 1 (reset (+ 10 (shift k 100))))
  => 110
(+ 1 (+ 10 (reset (shift k 100))))
  => 111
\end{verbatim}

However, note that when we say ``between'' the shift and nearest reset, we do not mean the program text between the special forms.
Instead, we mean the continuation is defined by the dynamic process that evolved between the time when \texttt{reset} was encountered and when \texttt{shift} was encountered during evaluation.
This is an essential feature of FCC which directly leads to its expressive power.
The next examples split \texttt{shift} and \texttt{reset} into separate parts of the program.

\begin{verbatim}
(define (push-inc-continuation thunk)
    (reset + 1 (thunk)))
(define (with-last-continuation thunk)
    (shift k (thunk k)))

(push-inc-continuation (lambda ()
    (with-last-continuation (lambda (k) 10))))
  => 10
(push-inc-continuation (lambda ()
    (with-last-continuation (lambda (k) (k 10)))))
  => 11
\end{verbatim}

Before moving on to a much less trivial example, we would like to briefly examine an informal operational semantics for evaluating expressions under FCC.
As we evaluate an expression, we also maintain a control stack.
The control stack is a stack mostly of contexts, each of which is an expression with exactly one subexpression replaced with a hole, written $\square$.
During evaluation, complex expressions under consideration are recursively split into two parts: a context and a simpler sub-expression.
The context is pushed onto the stack, and evaluation continues with the subexpression.
When we reach a suitably simple expression, we reduce it to a value, then pop the topmost context.
The hole in this context is replaced with the value, forming an expression on which evaluation continues.
When we have a value but no remaining contexts in the stack, evaluation is complete.
A small example is given in Figure~\ref{fig:context-stack-addition} for concreteness.

\begin{figure}[H]
\caption{Context-Stack Addition}
\label{fig:context-stack-addition}
\begin{tabular}{ll}
\textbf{Current Subexpression} & \textbf{Context Stack} \\
\texttt{(+ 1 (* 2 3))} & \\
\texttt{(* 2 3)} & \texttt{(+ 1 $\square$)} \\
\texttt{(+ 1 6)} & \\
\texttt{ => 7} & \\
\end{tabular}
\end{figure}

To extend this language with first-class control, we add a stack mark, written $\#$, which may also be pushed to the stack.
Whenever we have a value, but a mark is on top of the stack, we pop the topmost marks and continue from there.
There are then two special forms:
the \texttt{reset} form pushed a stack mark, and the \texttt{shift} form aborts and reifies a portion of the control stack, then binds the result in the body and continues evaluation.
The portion of the stack captured extends from the top of the stack up to (but not including) the nearest stack mark.
If there is no such mark, then the entire stack is captured.
During reification, we form an expression by iteratively substituting contexts from this captured stack segment into the hole of the next context down, but replace the hole in the bottommost context with a fresh variable, $x$.
When this is done, we wrap the resulting expression in a \texttt{reset} form and also a lambda which binds $x$, thus obtaining the reified continuation.
So for example, if the stack looks like [\texttt{(+ 1 $\square$)}, \texttt{(+ 2 $\square$)}, $\#$, \texttt{(+ 3 $\square$)}], then evaluation of a shift form would leave [$\#$, \texttt{(+ 3 $\square$)}] on the stack, and the captured continuation would be \texttt{(lambda (hole) (reset (+ 1 (+ 2 hole))))}.

As an example, Figure~\ref{fig:simple-fcc-reduction} evaluates \texttt{(reset (+ 1 (shift k (k 10))))} to normal form.
In the first step, we encounter a reset, so we push a stack mark.
In the second, we see a primitive operator applied to a number and a complex expression, so we push a context and move to the complex expression.
In the third step, we encounter a shift, so we create a lambda form by popping the topmost section of stack.
At that point, we essentially have a standard lambda calculus term left, so evaluation proceeds without note.
That the captured continuation includes a reset form is not essential to first-class control in general, but it is part of the definition of the shift-reset system.

\begin{figure}[H]
\caption{Simple FCC Reduction}
\label{fig:simple-fcc-reduction}
\begin{tabular}{ll}
\textbf{Current Subexpression} & \textbf{Context Stack} \\
\texttt{(reset (+ 1 (shift k (k 10))))} & \\
\texttt{(+ 1 (shift k (k 10)))} & $\#$ \\
\texttt{(shift k (k 10))} & \texttt{(+ 1 $\square$)}, $\#$ \\
\texttt{(($\lambda$ (hole) (reset (+ 1 hole))) 10)} & $\#$ \\
\texttt{(reset (+ 1 10))} & $\#$ \\
\texttt{(+ 1 10)} & $\#$, $\#$ \\
\texttt{ => 11} \\
\end{tabular}
\end{figure}


We will now examine a more interesting example, which implements a loop with early-exit.
First, recall the definition of \texttt{foldl}, which reduces a list from left-to-right according to the passed function.
Two example uses of \texttt{foldl} are given as well.

\begin{verbatim}
;; loop over a list, accumulate a value
(define (foldl f xs acc)
    (if (= (length xs) 0)
        acc
        (foldl f (cdr xs) (f acc (car xs)))))

;; foldl can be used like this:
(define (sum numbers) (foldl + numbers 0))
(define (product numbers) (foldl * numbers 1))
\end{verbatim}

We can very cleanly define a function that returns true only when every element in a list satisfies some predicate.
However, doing so can be wildly inefficient, because \texttt{foldl} always consumes every element in the list.

\begin{verbatim}
(define (slow-all predicate items)
  (define (test x acc)
    (if (predicate x)
        acc
        false))
  (foldl test items true))
  
;; we already know the answer by the second element
;; but foldl will just keep testing all the remaining numbers
(slow-all (lambda (x) (> x 0)) '(1 -5 2 3 ...))
\end{verbatim}

Instead, we would like is to perform early-exit.
We can do this using shift-reset, and even re-use our old \texttt{foldl} function.
Here, \texttt{foldl/ee} wraps the call to \texttt{foldl} in a \texttt{reset} form, but there is no \texttt{shift} form immediately present.
Instead, our definition of \texttt{all} will arrange for \texttt{break} to be called if we should find an item that does not satisfy the predicate.
This implementation of \texttt{all} will not check any further items once a counter-example has been found.
Figure~\ref{fig:foldl/ee-reduction} shows an outline of the reduction steps during evaluation.
Alternately, the reader may confirm this with a Racket debugger.

\begin{verbatim}
(define (foldl/ee f xs acc)
  (reset (foldl f xs acc)))
(define (break ans) (shift k ans))

(define (all predicate items)
  (define (test x acc)
    (if (predicate x)
        acc
        (break false)))
  (foldl/ee test items true))
\end{verbatim}

\begin{figure}[H]
\caption{foldl/ee Reduction Sketch}
\label{fig:foldl/ee-reduction}
\begin{tabular}{ll}
\textbf{Current Subexpression} & \textbf{Context Stack} \\
\texttt{(all ($\lambda$ (x) (> x 0)) '(0 1))} & \\
\texttt{(foldl/ee test '(0 1) true)} & \\
\texttt{(reset (foldl test '(0 1) true))} & \\
\texttt{(foldl test '(0 1) true)} & $\#$ \\
$\!\!\!\!\!$
\begin{tabular}{l}
	\texttt{(foldl test '(1) } \\
	$\;\;\;\;$\texttt{(test true (car '(0 1))))} \\
\end{tabular}
  & $\#$ \\
\texttt{(test true 0)} & \texttt{(foldl test '(1) $\square$)}, $\#$ \\
$\!\!\!\!\!$
\begin{tabular}{l}
	\texttt{(if (($\lambda$ (x) (> x 0)) 0)} \\
	$\;\;\;\;$\texttt{true} \\
	$\;\;\;\;$\texttt{(break false))} \\
\end{tabular}
	& \texttt{(foldl test '(1) $\square$)}, $\#$ \\
\texttt{(($\lambda$ (x) (> x 0)) 0)}
& $\!\!\!$\begin{tabular}{l}
	\texttt{(if $\square$ true (break false))}, \\
	$\;\;\;\;$ \texttt{(foldl test '(1) $\square$)}, $\#$ \\
\end{tabular} \\
\texttt{(> 0 0)}
& $\!\!\!$\begin{tabular}{l}
	\texttt{(if $\square$ true (break false))}, \\
	$\;\;\;\;$ \texttt{(foldl test '(1) $\square$)}, $\#$ \\
\end{tabular} \\
\texttt{(if false true (break false))} & \texttt{(foldl test '(1) $\square$)}, $\#$ \\
\texttt{(break false)} & \texttt{(foldl test '(1) $\square$)}, $\#$ \\
\texttt{(shift k false)} & \texttt{(foldl test '(1) $\square$)}, $\#$ \\
\texttt{false} & $\#$ \\
\texttt{ => false} \\
\end{tabular}
\end{figure}

Although our purpose in this section was only to build some intuition, the reader may now have several questions regarding FCC.
In particular, shift-reset does not lend itself well to modularity, nor have we developed a significant example demonstrating the power of FCC beyond exceptions.
The FCC introduced here was particularly simple flavor, though it has seen implementation in a production language.
In the following sections, we will examine several flavors of FCC in more depth.
Once we have developed $\lambda_{SFCC}$, then we will return to more significant examples of use.


\maybePage
\subsection{Undelimited Continuations}
\label{undelimControl}

We earlier defined a continuation as the stack of computations that we will need to perform after simplifying a subexpression.
In fact, this stack in its entirety is called an \emph{undelimited continuation}.
In contrast, a \emph{delimited} continuation, which we will examine next, is only a portion of the undelimited continuation.
However, undelimited continuations alone cannot implement arbitrary control structures.
Instead, a lambda calculus must also be extended with mutable state \cite{Filinski94}, though this is admittedly not a barrier in production languages, even those which already include exceptions \cite{GreatEscape}.

Though powerful, undelimited continuations have been found to be a poor choice in comparison with delimited continuations \cite{MFDC}\cite{continuationsInProcObjs}\cite{HandlingControl}\cite{Sitaram90}.
Understanding undelimited continuations is only necessary so that we can avoid the flaws in their design, and to set the stage for what follows.
We will therefore end this section by summarizing a few of the arguments against undelimited control most relevant to our purpose.

Note that any subprogram with the ability to manipulate undelimited continuations can trivially take total control over and manipulate the entirety of the remaining process\footnote{In fact, a buffer overrun attack that leads to arbitrary code execution can be seen as a form of \texttt{call/cc}.}.
This is in fact the definition of undelimited control, but it is also a prescription for arbitrary code injection.
Undelimited continuations are thus a very easy pathway to hijack a program, whether by accident or malicious intent.
As such, they cannot serve as a basis for secure or reliable programming systems.

Furthermore, although undelimited continuations are capable of simulating arbitrary control flows, the process is difficult and subtle.
In general, such simulation requires whole program transformation \cite{Filinski94}, and as such is really only of interest to compiler writers.
Indeed, as far as the user-level language is concerned, undelimited continuations are not able to fully abstract control flow \cite{Sitaram90}.

More generally, it is important to note that delimited continuations are often called ``composable'' to distinguish them from undelimited continuations, which are not directly composable.
Non-composable operations are expensive to maintain, when they can be maintained at all.
From a software engineering perspective, then, we must avoid undelimited control.


\maybePage
\subsection{Delimited Continuations}
\label{delimControl}

One difficulty of describing FCC is that there are so many different variants in the literature.
We will consider here three broad groups of delimited control operators.
In the following, we will often make reference to academic literature;
lest the reader think we have abandoned our focus on production programming languages, note that all of these operators have been implemented in Racket \cite{RacketReference}.
In fact, every delimited control operator in Racket is either presented here directly, or else can be easily simulated.

In what follows, we will see that Racket implements a veritable zoo of control operators.
There are of course additional delimited control operators available in the literature, often with unique features of questionable utility.
It would be preferable of course to agree on only a few operators, and this is perhaps the largest single issue with Racket's system of control: there is no unifying principle motivating the inclusion or exclusion of these operators, beyond what happens to be easily implementable.

\maybePage
\subsubsection{$\pmFpm$}

The form of delimited control introduced in \ref{subsec:intro-to-fcc} is called \textit{shift-reset}, and is distinguished by the fact that the reified continuation re-installs a stack mark, but the abort does \emph{not} remove the delimiting stack mark.
Another form, called \emph{prompt-application}, is defined in \cite{PromptApplication} which is the same system, except that the reified continuation does \emph{not} re-install the stack mark\footnote{In \cite{PromptApplication}, what we call stack mark is called a ``prompt.''}.

In fact, prompt-application was the first of four variants which differ only in whether the continuation-capture operator would remove the stack mark on abort and whether a reified continuation would re-install it.
After \cite{MFDC}, we will call the collection of these systems $\pmFpm$.
The terminology derives from \cite{PromptApplication}, which wrote the capture operator as $\mathcal{F}$.
In each system, the operator to install a stack mark---called \texttt{reset} in \ref{subsec:intro-to-fcc} and written $\#$ in \cite{PromptApplication}---is the same.
The systems differ in the behavior of there control operator; these differences are represented by the choices between plus and minus:
\begin{itemize}
\item $^+\mathcal{F}^-$ is the delimited $\mathcal{F}$ operator in \cite{PromptApplication}. It leaves the stack mark behind, and does not include it in the continuation.
\item $^+\mathcal{F}^+$ is called \emph{shift} from \cite{AbstractingControl}, an in our Sec.~\ref{subsec:intro-to-fcc}. It leaves the stack mark behind, but includes it in the continuation.

\item $^-\mathcal{F}^+$ is similar to the \emph{spawn} operator in \cite{ContinuationsAndConcurrency}, though we will see an exact implementation in ???. It removes the stack mark, but includes it in the continuation.
\item $^-\mathcal{F}^-$ is similar to \emph{cupto} from \cite{Gunter:1995}, we will examine next. It removes the stack mark, and also includes it in the continuation.
\end{itemize}
A more in-depth discussion of the properties of this system is available in \cite{MFDC}.

\maybePage
\subsubsection{cupto}
\label{subsec:cupto}

So far, we have considered control operators that can capture and abort only up to the nearest stack mark.
In fact, systems based only on these operators suffer from a lack of composability.
Consider the implementation of \texttt{foldl/ee} implemented in Sec.~\ref{subsec:intro-to-fcc}; if we were to nest calls to \texttt{foldl/ee}, the inner call would be unable to break out of the outer call.
In major imperative languages, by contrast, it is simple to break out from nested loops.
The depth of the situation we face here is most readily seen if we consider the analogy from \cite{ContinuationsAndConcurrency}: ``It is as if we were programming in a block-structured language that restricts us to one variable name.''

To create fully abstract control structures, we need a supply of distinct stack marks, and some way to obtain a fresh mark.
Our control operators will then take an additional parameter determining which mark they work with.
Just such a system is developed in \cite{Gunter:1995}.
Importantly, it introduces the primitive \textbf{newPrompt}, which obtains a fresh stack mark.
Instead of \texttt{reset}, we use $\textbf{set }p\textbf{ in }e$, where $p$ should evaluate to a stack mark, to push $p$ before evaluating $e$.
Instead of the $\pmFpm$ operators, we use a $\textbf{cupto }p\textbf{ as }k\textbf{ in }e$ operator, which captures and aborts up to the nearest $p$ mark (bypassing other marks), and binds $k$ to the captured continuation in $e$.
With \textbf{cupto}, the stack mark is neither left behind, nor is it reinstalled, as in $^-\mathcal{F}^-$.
However, the authors do not express a strong opinion on the flavors presented by $\pmFpm$, and in fact note that it is easy to simulate other flavors starting from $^-\mathcal{F}^-$.

\maybePage
\subsubsection{fcontrol}

In the operators so far, the control operator has not only been responsible for effecting continuation capture and abort, but is also responsible for combining the reified continuation with any other required values which must be available locally.
In contrast, exception mechanisms bundle up any local values into an exception, but the control flow after the abort is specified by the delimiter---a catch block.
In \cite{HandlingControl}, Sitaram generalized try-catch to include continuation capture, introducing the control operator \texttt{fcontrol} and the corresponding delimiting operator \texttt{run}.
This system, apart from specifying the handler on a delimiter and therefore requiring any required local data to be packaged with the control operator, is essentially the same as those discussed above, and so is also subject to the same range of variation.

\maybePage
\subsubsection{Analysis}
\label{subsubsec:delimControlAnalysis}

The reader might surely wonder at this point if there are further variants.
Indeed, there are, but these are either easily simulated by the above systems, or else are so unique as to be of questionable utility.
Given the possibilities we have discussed, we see several design choices:
\begin{enumerate}
\item whether to remove the delimiting stack mark on abort,
\item whether to include the stack mark in the captured continuation,
\item whether to provide a supply of fresh stack marks, or else allow only a single mark, and
\item whether to specify the handler with the control operator or with the delimiting operator.
\end{enumerate}
While these are orthogonal, certain choices are more expressive than others.
As already mentioned, choosing to remove the delimiting mark but not include it in the captured continuation allows simple simulation of the other three possibilities in this space.
Further, providing a supply of fresh stack marks allows us to use multiple control structures at once without being concerned with adverse interactions.

The choice in (4) has no immediately clear answer.
It seems in some programs there is sufficient information near the control operator to proceed with the computation; in other systems, that information might only exist near the delimiting operator.
Although the focus in FCC research has been into the former systems, exception handling certainly falls into the latter category.

There is one wrinkle in the choice of (3) to allow the user the ability to create stack marks on the fly
With first-order FCC, we can implement finally, or any other control structures which intercept non-local control flow.
However, when we have dynamically-generated prompts, all a code fragment has to do is abort to a prompt that is not recognized by a finalizer, e.g. any freshly-obtained prompt.
The introduction of dynamic prompts thus requires an additional mechanism to implement finalizers and other control-flow interception mechanisms.

There is one additional design choice, related to the misuse of continuations.
What should happen when a program attempts continuation capture, but does not encounter any matching delimiting stack mark?
In Racket, a call to shift without a corresponding reset operates by capturing the entire continuation, rather than a subcontinuation.
It is this author's opinion that, since undelimited control is almost never intended, undelimited continuation capture should always be an error.
A similar error occurs when an exception is thrown but unhandled: the program crashes and a stack trace may be printed.
Indeed, it would be simple when a continuation capture is undelimited, to terminate the program, possibly with a stack trace.
The choice to terminate follows the ``catch errors early'' principle, and  has proven effective in a vast number of exception implementations, whereas delimited control has only a few implementations.

\maybePage
\subsection{Dynamic Wind}
\label{dynamic-wind}

Dynamic-wind is another simple concept, which is broadly a generalization of try-finally.
Dynamic-wind is a form which takes three blocks\footnote{Technically, it takes three thunks, but there's really no good reason to introduce the boilerplate \texttt{(lambda () ...)} all over the place. So, we will speak as if the arguments were passed unevaluated, as if to a special form.} of code: a before-, body-, and after-block.
Control then attempts to enter the body-block, but whenever the body is entered, whether initially or because it is placed back onto the stack, the before-block is executed first.
Furthermore, no matter how control exits the body, the after-block will be evaluated.

%TODO where multiple prompts alone cannot guarantee setup or cleanup when a dynamic extent is entered/exited, the addition of dynamic-wind gives us this capability

It should be clear then, that try-finally is a limit case of dynamic-wind, where the before-block is empty.
Conversely, dynamic-wind generalized try-finally by not only allowing code to be registered for execution on exit, but also on entrance.
Try-finally is sufficient to manage cleanup of resources in the presence of non-local exit, but can manage acquisition of resources under only local entry.
Dynamic-wind allows the programmer to ensure that both setup and cleanup code are properly executed even under non-local control flow such as that introduced by FCC.
Since the two mechanisms are so similar, they share essentially the same advantages and disadvantages.
In any case, we will not repeat our analysis here.




\maybePage
\section{Review}
\label{sec:review}

We have now examined a wide range of control flow operators from eight independent programming languages in several paradigms.
In this section, we will summarize the properties which were found effective at resource management and those which were not.

The use of the stack to effect cleanup is effective and deterministic.
RAII (\S\ref{RAII}), try-finally (\S\ref{try-finally}) and defer statements (\S\ref{defer}) all illustrate this.
In contrast, we cannot tie cleanup to the garbage collector since it is non-deterministic, and manual, i.e. error-prone, cleanup is precisely what we are seeking to avoid.
More subtly, an excessively lazy evaluation strategy eliminates linear nature of the stack, eroding the determinism of stack-based constructs.

The re-use of resource management code, as in RAII (\S\ref{RAII}) and context statements (\S\ref{context statements}) helps reduce code duplication.
At the same time, however, some management code is only used once, in which case it is best to give the code in-line, rather than introduce additional boilerplate.
Thus, we must be able to give resource management code both directly at the usage site as well as in a re-usable form.

Most control structures that intercept non-local control flow also intercept local control flow.
Nevertheless, the scope statements in D (\S\ref{defer}) and monads (\S\ref{subsec:exceptionMonads}) are able to discriminate between local and non-local exit\footnote{Since monads can only simulate non-local control flow, their ability to distinguish relies on encoding control flow into data.}.
Although we did not discuss it in the text, Python is also able to detect local exit with the try-else construct.\cite{pythonDocs}
The ability to take different actions depending on the nature of control flow is useful for software transactions.

The same way try-catch-finally (\S\ref{try-finally}) can be straightforwardly audited for exception safety, we would like our language to be easily checked for the existence of non-local control flow.
In the languages we have discussed so far, there has only been one operator which initiates non-local exit.
With these constraints, it is simple to document any functions which transitively rely on control operator, or to encode this property with the type system, if possible.
With the exception of call/cc (\S\ref{undelimControl}), non-local entrance to indistinguishable from a function call, so as long as we constrain ourselves to composable continuations, we need not specially note when captured continuations are restored.


we must be able to delimit control operators
separate control structures will need different delimiters in order to stay out of each other's way


Good features found:
%  1) stack-allocated memory can be cleaned deterministically,
%  2) directly (i.e. inline) install deterministic handlers (e.g. catch-clauses),
%  3) reuse setup/cleanup pairs (well, reuse more generally, but...)
%  4) straightforward or (better) automatic auditing for exception safety,
  5) cleanup must be executed no matter how control exits,
  6) corollary: setup must be executed no matter how control enters,
%  7) discriminate between normal and abnormal exit
Features to avoid:
%  1) manual management of heap-alloc'd memory,
%  2) destruction/cleanup tied to garbage collector,
%  3) boilerplate per method of setup/cleanup,
  4) manual translation to continuation-passing style,
%  5) indiscriminate laziness,
  6) codifying any specific patterns (other patterns are always left un-codified, but if we let the user codify patterns himself, no pattern need be codified),
Initial goals were to promote efficient code re-use in favor of pattern-based programming.
We won't worry about syntactic abstraction, as that's independent and has several possible solutions, though our demonstration langauge does have it.


%Each system discussed above has severe problems when used as the sole method of resource management. Even combining all these techniques into a single language is insufficient to overcome the problem of abstracting control flow without resorting to explicit continuation-passing.

%Abstraction is limited in all of these schemes. RAII and with/using are only able to abstract patterns of execute-around. Try/finally and scope statements are not capable of any abstraction, short of manually passing continuations.

%RAII: abstractible, unsafe or non-deterministic, verbose, directly applies only to physical resources
%try-finally: non-abstractible, determistic, applies generally
%with/using: abstractible, deterministic, verbose, directly applies only to physical resources
%scope statements: non-abstractible, concise, deterministic, applies generally
%monads: abstractible, deterministic, concise, poor performance characteristics, applies generally
%
%although user-implemented monads offer poor performance characteristics, the type system is very good at checking exceptions in a non-annoying way.
%before control exits from blocks, some code must be executed: usually implemented by marking the stack.
%allow for transactions by distinguishing between normal/abnormal exit.
%
%
%
%from exceptions: determinism, mark the stack with handlers, mark the stack with finalizers, abstraction
%
%from fcc: reify/replace parts of the stack, mark stack fragments with initializers, abstraction
%
%use a rich type system to check exception safety
%
%find a place for this: it is commonly claimed that exceptions can be implemented with continuations + mutable state. Unfortunately, this strategy is not very pure




MEMOS:
\begin{itemize}
\item `syntactic theory of fcc' could not give higher praise to fcc as a implementation technique
\end{itemize}



\maybePage
\part{Thesis}

Our thesis begins with the presentation of an extended lambda calculus designed for secure first-class control, which we call $\lambda_{SFCC}$.
We then briefly examine its relationship to a few important existing control structures.
Next, we present an operational semantics for $\lambda_{SFCC}$ in the form of an interpreter, and note additional efficiencies that may be garnered with an implementation.
Finally, we examine a wide range of control structures, both mundane and advanced, and show implementations of these in our calculus.

As in most discussions of formal semantics, we will be working simultaneously with syntax, semantics and metasyntax. To be unambiguous, we will typeset each level differently. Metasyntax will be in \textit{italics}, semantic objects will be written in \textbf{boldface}, and syntax will be written either in \textsf{sans-serif} for keywords or in \texttt{typewriter} for variable names.





\maybePage
\section{The SFCC Calculus}
\label{sec:SFCCcalculus}

In the design of SFCC, our guiding principle has been in relation to the four parameters mentioned in \ref{subsubsec:delimControlAnalysis}:
at each step, we have chosen notions which are able to implement either choice.
The techniques we use are drawn primarily from four existing control concepts:
the system $\pmFpm$ as presented in \cite{MFDC}, the \texttt{fcontrol} operator from \cite{HandlingControl}, dynamic-wind from Scheme, and the ability to discriminate normal and abnormal exit from D.

As regards our delimiting and control operators, the key idea is to specify handlers on both the delimiting operator (as in fcontrol) and on the control operator (as in most other systems).
We automatically invoke the operator's handler, but control may, from there, be given to the delimiter's handler.
Essentially, either the control operator has enough information to move the computation along on its own, or else it has enough information to know that it must seek outside help from the delimiter's handler.
For the management of resources, the key insight is that there are only four forms of control flow, each of a distinct character: entrance vs. exit and local vs. non-local.
Thus, all we need to do is to execute an additional expression (the setup or clean up code) whenever control moves past a guard.

We will present the concepts of $\lambda_{SFCC}$ in stages, each building on the last.
We have taken care so that the presentation of each system will remain valid we extend it with additional concepts.
When we have developed SFCC in full, we give the system as a whole, so that readers may skip over the intermediate stages of development.
Our system was developed independently of the requirements summarized in \S\ref{sec:review}, but we will show that it does meet those standards.
This sort of demonstration is interesting because it shows that the power of the system extends beyond its direct influences.

\subsection{Base Calculus}

We begin our work with $\lambda_v$, the call-by-value lambda calculus.
We have chosen call-by-value specifically so that we can easily control the order of evaluation, esp. to obtain determinism.
To briefly review, we give the grammar in Figure~\ref{fig:lambdaCalculus}.
The notation $[x \backslash e']e$ is used for capture-avoiding substitution.
The only reduction rule is $\beta$-reduction:
$$(\x.e)\;v \mapsto [x \backslash v]e$$

\begin{figure}[H]
\caption{Grammar of $\lambda_v$ Calculus}
\label{fig:lambdaCalculus}

\begin{tabular}{llclll}
Values & $v$ & ::= & $(\lambda x.e)$ && abstraction\\
Expressions & $e$ & ::= & \\
& & $|$ & $v$ && \\
& & $|$ & $x$ && variable \\
& & $|$ & $(e\;e)$ && application \\
\end{tabular}
\end{figure}


As in most other presentations, we freely drop parenthesis where doing so would be unambiguous.
We also include a few common syntactic niceties.
\begin{itemize}
\item Adjacent lambdas may be written as a single lambda.
	$$\x_1,\ldots,x_n.e \equiv \x_1.\x_2\ldots\x_n.e$$
\item Lambdas may discard their argument.
	$$\lambda\_.e \equiv \x.e \quad\textrm{ where }x \notin fv(e)$$
\item Expressions may be sequenced; semicolon is right-associative.
	$$e_1; e_2 \equiv (\lambda\_.e_2)\;e_1$$
\item Let expressions increase readability,
	$$\letin{x = e'}{e} \equiv (\x.e)\;e'$$
\item and may make multiple bindings in sequence.
	$$\letin{x_1 = e_1; \ldots; x_n = e_n}{e} \equiv \letin{x_1 = e_1}{\ldots\letin{x_n = e_n}{e}}$$
\item We will occasionally have need of ``zero-argument'' lambdas.
These are really no different than lambdas which discard their argument, which should not have required any work to compute.
We nevertheless include syntax for them, so as to better express our intentions.
	$$\lambda().e \equiv \lambda\_.e$$
	$$e\;() \equiv e\;v$$
\end{itemize}


\maybePage
\subsection{Continuation Values}

In our first extension, we add the notion of a subcontinuation---a reified continuation.
The grammar is given in Figure~\ref{fig:addSubconts}.
It is clear from structural induction that every subcontinuation has exactly one\footnote{Subcontinuations may nest, but we treat these as simple values rather than deconstructible syntax trees, so we don't count holes that appear underneath angle brackets.} $\square$, also called a hole.
Subcontinuations are always defined such that replacing this hole by an expression yields an expression.
We denote the replacement of the hole in a subcontinuation $K$ by an expression $e$ as $K[e]$.

\begin{figure}[H]
\caption{Extend with Subcontinuations}
\label{fig:addSubconts}

\begin{tabular}{llclll}
Values & $v$ & ::= & $\ldots$ \\
& & $|$ & $\angles{K}$ && subcontinuation \\
Continuations & $K$ & ::= & \\
& & $|$ & $\square$ && hole \\
& & $|$ & $K\;e$ &&  \\
& & $|$ & $v\;K$ &&  \\

\end{tabular}
\end{figure}

Subcontinuations purposely bear a remarkable resemblance to functions.
They essentially are functions of one variable, where that variable is only used once in the function body.
Unlike lambda-introduced functions which take their argument by value, subcontinuations take their argument by-name.
To this end, we add the following reduction rule so that application is productive for both functions and subcontinuations:
$$\angles{K}\;e \mapsto K[e]$$

A dedicated notion of subcontinuation---as distinct from function---is important so that the calculus is able to manipulate the continuation before proceeding with other work.
This will become particularly important for several control structures we implement later, esp. a control operator for continuation capture without abort.

Readers might note that Felleisen in \cite{PromptApplication} models continuations using lambdas in continuation passing style.
This reduces the size of his grammar, but at the cost of the presence of difficult to untangle values, such as $\lambda z.(\lambda y.(\x.x)\;(1+y))\;(f\;z)$, which is simply $\angles{1 + f\;\square}$ in our notation.
As we will be regularly working with continuations, we have opted for a calculus which is easy to read, even at the cost of additional nonterminals and productions in the grammar.
Nevertheless, Felleisen clearly shows that this distinction is merely cosmetic.


\maybePage
\subsection{First-class Control}

We now move to the core of the system---the control and delimiting operators---which will require several interlocking notions:
those of prompt creation, and delimiting and control operators.
The grammar is given in Figure~\ref{fig:addControl}.

\begin{figure}[H]
\caption{Extend with Control Operators}
\label{fig:addControl}

\begin{tabular}{llclll}
Values & $v$ & ::= & $\ldots$ \\
& & $|$ & $p$ && prompt \\
Expressions & $e$ & ::= & $\ldots$ \\
& & $|$ & $\nu$ && new prompt \\
& & $|$ & $\#_{e_p}^{e_h}e$ && delimiting operator \\
& & $|$ & $\F_{e_p}^Ke$ && control operator \\
Continuations & $K$ & ::= & \\
& & $|$ & $\#_{K}^{e_h}e$ &&  \\
& & $|$ & $\#_{p}^{K}e$ &&  \\
& & $|$ & $\#_{p}^{v}K$ &&  \\
& & $|$ & $\F_{K}^{\square}e$ &&  \\
& & $|$ & $\F_{p}^{\square}K$ &&  \\
\end{tabular}
\end{figure}

The last two productions for $K$ now include a $\square$ superscript in addition to the hole which appears in the $K$ non-terminal of those productions.
We recover the existence of a single salient hole by ignoring the superscript in these productions as we search for the hole.
Essentially, the superscript is a bookkeeping device rather than a real part of an expression.
We can do this because we now also restrict programs so that they not only contain no free variables, as is normal in the lambda calculus, but they also may only have $\square$ as the superscript of any $\F$ expression.
Since the presence of the $\square$ increases line noise without contributing much information, we will often elide it:
$$\F_{e_p}e \equiv \F_{e_p}^{\square}e$$
We will now turn to discussion of the three interlocking subsystems of this extension.

First, we have decided to allow for both control and delimiting operators to be tagged as in \S\ref{subsec:cupto}.
To do this, we will need a new category of values---prompts---which we write with the metavariable $p$.
Further, we need an operator with which we can create a new prompt value on demand, which we write $\nu$.
This will require a crosscutting change to our evaluation procedure.
Where before we simply reduced one expression to another, evaluation will now need to track which prompts are in use.
To do this, we use relations of the form $e/P \mapsto e'/P'$, where $P$ is a set of prompts.
We need not restate our previous reduction rules, however, as we can simply make the following identification:
$$e \mapsto e' \equiv e/P \mapsto e'/P$$
which is to say, we can leave out the specification of the set of prompts when it is neither inspected nor altered.

With this in mind, the reduction rule for $\nu$ is
$$\nu/P \mapsto p/P\cup\{p\} \quad\textrm{where }p \notin P$$
Unlike other reduction rules, this one cannot be used underneath an abstraction.
Further, a if $e$ contains a $\nu$ subexpression, multiple copies of $e$ in the same program will produce different results.
Thus, each application of $\lambda().\nu$ produces a distinct value.


After \cite{PromptApplication}, the delimiting operator is written $\#$ and the control operator written as $\F$.
Both enclose an expression called the body, which is written to the right of the operator.
For the delimiting operator, the body is within the dynamic extent of the operator, and for the control operator, the body should evaluate to a handler.
Both also take an expression, written as a subscript, which should reduce to a prompt. Through that, a control operator is able to identify a matching delimiter.

The reduction rule for delimiters simply allows a value to pass through it without modification:
$$\#_p^e v \mapsto v$$

Reducing a control operator is much more intricate, however, as it involves delimited continuation capture.
Control operators begin reduction when their body is reduced to a value.
They recurse up the expression until they match an appropriate delimiter.

The base rule matches a control operator tagged with a prompt to an immediately enclosing delimiter tagged with the same prompt.
At that point, we apply the handler and the captured continuation to the value passed to the control operator.
$$\#_p^{v_h}\F_p^Kv \mapsto v\;v_h\;\angles K$$

We now need reductions which allow a control operator to walk up an expression, capturing a continuation.
We will need one reduction rule for each place an expression occurs in each production.
While this means there will need to be many rules, they all follow the same pattern:
we keep the control operator, but move the surrounding expression into the control operator's captured continuation, replacing where the control operator was with the previously captured continuation.

$$(\F^K_pv)\;e_2 \mapsto \F^{K\;e_2}_pv$$
$$v_1\;\F^K_pv \mapsto \F^{v_1\;K}_pv$$
$$\#_{\F_p^Kv}^{e_h}e \mapsto \F_p^{\#_K^{e_h}e}v$$
$$\#^{\F_p^Kv}_{p'}e \mapsto \F_p^{\#^K_{p'}e}v$$
$$\#_{p'}^{v_h}\F^K_pv \mapsto \F^{\#_{p'}^{v_h}K}_pv
  	\quad\textrm{where }p \neq p'$$
$$\F_{\F^K_pv}e \mapsto \F^{\F_Ke}_pv$$
$$\F_{p_1}\F^K_{p_2}v \mapsto \F^{\F_{p_1}K}_{p_2}v$$

\maybePage
\subsection{Extent Guards}
\label{subsec:extentGuards}

Having now introduced control operators, we turn to the other main part of the system---the operators which can ensure correct setup and cleanup of resources.
These we call extent guards, because they register expressions which are then executed when control enters/exits the dynamic extent of the guard.
We supply enough guards to recognize local and non-local control flow, or either.
The extent guard concept can be approached in any of several ways, two of which we will now give.

First, they are an extension of D's scope statements.
Where scope statements can be triggered on normal, abnormal, or any exit, extent guards can also be triggered on normal, abnormal or any entrance.

Second, extent guards may be seen as an atomization of the aspects of dynamic-wind.
In dynamic-wind, code for both entrance and exit are specified at one, and these codes are executed regardless of whether the entrace/exit was normal or abnormal.
The entrance and exit extent guards can be specified independently of the other, and can also discriminate between normal and abnormal jumps.

To help keep track of these six extent guards, we use mnemonics in our notation.
Those guards which are executed on entrance are represented with upwards arrows, which those executed on exit with downwards arrows; this is in keeping with an upward-growing stack convention.
Extent guards executed on local control flow are represented with a simple arrow ($\uparrow$), those executed on non-local control flow with a broad arrow ($\Uparrow$), and those executed on any control flow with double arrows ($\upuparrows$).

\begin{figure}[H]
\caption{Extend with Extent Guards}
\label{fig:addGuards}

\begin{tabular}{llclll}
Expressions & $e$ & ::= & $\ldots$ &  \\
& & $|$ & $\#_\Uparrow^{e_g}e$ && non-local entrance guard \\
& & $|$ & $\#_\Downarrow^{e_g}e$ && non-local exit guard \\
Continuations & $K$ & ::= & \\
& & $|$ & $\#_{\Uparrow}^{e}K$ &&  \\
& & $|$ & $\#_{\Downarrow}^{e}K$ &&  \\
\end{tabular}
\end{figure}

In the grammar, given in Figure~\ref{fig:addGuards}, we only add the forms for guards triggered on non-local control flow.
We do this to reduce the size of our calculus, recognizing that the others are easily simulated:
$$\#_\uparrow^{e_g}e \equiv e_g; e$$
$$\#_\downarrow^{e_g}e \equiv \letin{x=e}{e_g; x}$$
$$\#_\upuparrows^{e_g}e \equiv \#_\uparrow^{e_g}\#_\Uparrow^{e_g}e$$
$$\#_\downdownarrows^{e_g}e \equiv \#_\downarrow^{e_g}\#_\Downarrow^{e_g}e$$


For each new expression, we need two reduction rules.
Note that these only apply to the non-local extent guards specified in the grammar, not to the simulations.
First, the regular reduction rules
$$\#_{\Uparrow}^{e}v \mapsto v$$
$$\#_{\Downarrow}^{e}v \mapsto v$$
are a straightforward extension of the rule $\#_{p}^{e}v \mapsto v$ from the last section.

The next pair relate to continuation capture, and they each slightly break the pattern for continuation capture established in the last section.
First, examine the capture rule for non-local exit:
$$\#_{\Downarrow}^{e}\F_p^K v \mapsto e;\F_p^{\#_\Downarrow^{e}K}v$$
In addition to floating the control operator up the expression tree, this rule also inserts a copy of the guard expression to be evaluated before continuation capture can proceed.
The case for non-local entrance is somewhat more subtle:
$$\#_{\Uparrow}^{e}\F_p^K v \mapsto \F_p^{e;\#_\Uparrow^{e}K}v$$
Here, capture proceeds as normal, but the copied guard expression is inserted into the captured continuation.
The idea is that when the continuation is reinstated, the guard expression will be evaluated before another redex is found.

\maybePage
\subsection{Barriers}

Finally, we introduce one additional stack mark, which we call a barrier.
Barriers are introduced in order to delimit all control operators, regardless of their prompt.
We introduce these so that users and language designers have the option to insert them at any point where propagation of continuation capture would be in error, but we also generalize the ability to respond.
Instead of simply terminating the process (as when an exception propagates out from within a destructor in C++), a handler may be installed which can respond as appropriate in the situation.
The grammar is given in Figure~\ref{fig:addBarriers}.

\begin{figure}[H]
\caption{Extend with Barriers}
\label{fig:addBarriers}

\begin{tabular}{llclll}
Marks & $e$ & ::= & $\ldots$ &  \\
& & $|$ & $\#_\blacksquare^{e_h}e$ && barrier \\
Continuations & $K$ & ::= & \\
& & $|$ & $\#_{\blacksquare}^{K}e$ &&  \\
\end{tabular}
\end{figure}


There are two normal reduction rules.
The first should look familiar; it simply allows values to unwind past the barrier.
$$\#_\blacksquare^{v_h}v \mapsto v$$
The other reduction concerns an attempt to bypass the barrier with a control operator:
$$\#_\blacksquare^{v_h}\F_p^K v \mapsto v_h\;p\;\angles{K}\;v$$
In this case, the various values involved are diverted to the handler specified by the barrier.
Usually, the handler should simply initiate an error-handling mode, perhaps using its arguments to produce diagnostic messages.

Finally, there is a standard continuation capture rule:
$$\#_\blacksquare^{\F_p^K v}e \mapsto \F_p^{\#_\blacksquare^K e}v$$

Barriers are likely the most difficult expressions of this calculus to give a type.
Indeed, it may well be impossible to do so in any practical manner.
The formulation of barriers here is nevertheless the most general possibility for dynamically-typed languages.
If we were to adapt this concept to statically-typed languages, we need only recognize the original purpose for barriers: to prevent control from escaping the extent the barrier sets.
In statically-typed languages, we can restrict the barrier to either raise an appropriate exception or simply terminate the program immediately.
In this way, the handler is not dependent on the type of value and captured continuation obtained.
We will say more on this as we develop the applications of this system.

\maybePage
\subsection{Summary of the SFCC Calculus}
\label{subsec:summarySFCCcalculus}

This section collects the formal system of SFCC described above.
In addition, it applies somewhat more formalism in defining what reductions may be applied in what cases, thus giving a formal definition of equivalence.
We then collect the various syntactic abbreviations we have so far defined.
These do not extend the expressive power of the calculus in the sense of \cite{Felleisen90expressive}, and so we may regard these abbreviations as inessential to the theory;
nevertheless, their use greatly improves readability, which will be important when we apply our calculus.

We begin with the grammar, given in Figure~\ref{fig:SFCCgrammar}.

\begin{figure}[H]
\caption{Grammar of SFCC Calculus}
\label{fig:SFCCgrammar}

\begin{tabular}{llcl}
Values & $v$ & ::= & $\lambda x.e \mid \angles{K} \mid p$ \\
Expressions & $e$ & ::= &
          $v \mid
           x \mid
           e\;e$ \\
& & $|$ & $\nu \mid
           \#_{e_p}^{e_h}e \mid
           \F_{e_p}^Ke$ \\
& & $|$ & $\#_\Uparrow^{e_g}e \mid
           \#_\Downarrow^{e_g}e \mid
           \#_\blacksquare^{e_h}e$ \\
Continuations & $K$ & ::= &
          $\square$ \\
& & $|$ & $K\;e \mid v\;K$ \\
& & $|$ & $\#_{K}^{e_h}e \mid
           \#_{p}^{K}e \mid
           \#_{p}^{v}K$ \\
& & $|$ & $\F_{K}^{\square}e \mid
           \F_{p}^{\square}K$ \\
& & $|$ & $\#_{\Uparrow}^{e_g}K \mid
           \#_{\Downarrow}^{e_g}K \mid
           \#_{\blacksquare}^{K}e$ \\
\end{tabular}
\end{figure}


The equational theory is specified by a set of reduction relations.
A reduction relation takes the form
$$e/P \mapsto e'/P'$$
where $P, P'$ are sets of prompts and $R$ is a reduction rule, or set of such rules.
In the case where $P = P'$, we will elide mention of the prompt set:
$$e \mapsto e' \equiv e/P \mapsto e'/P$$


We now turn to the individual reduction rules of SFCC.
The primary reductions are those dealing with application:
$$(\x.e)\;v \mapsto [x \backslash v]e \qquad
  \angles{K}\;e \mapsto K[e]$$
There are then three rules dealing with the creation of prompts and the delimiting of continuation capture:
$$\nu/P \mapsto p/P\cup\{p\} \quad\textrm{where }p \notin P$$
$$\#_p^{v_h}\F_p^Kv \mapsto v\;v_h\;\angles K$$
$$\#_\blacksquare^{v_h}\F_p^K v \mapsto v_h\;p\;\angles{K}\;v$$
Next, two rules implement extent guards, which offer the ability to trigger expressions based on no-local control flow:
$$\#_{\Downarrow}^{e}\F_p^K v \mapsto e;\F_p^{\#_\Downarrow^{e}K}v \qquad
  \#_{\Uparrow}^{e}\F_p^K v \mapsto \F_p^{e;\#_\Uparrow^{e}K}v$$
These together comprise the interesting core of the system.

Four more rules are required to allow values to exit locally past various stack marks.
$$\#_p^{v_h} v \mapsto v \qquad
  \#_{\Uparrow}^{e}v \mapsto v \qquad
  \#_{\Downarrow}^{e}v \mapsto v \qquad
  \#_\blacksquare^{v_h}v \mapsto v$$

Finally, a number of rules are required to implement continuation capture.
Each of these rules follows a standard pattern, allowing the control operator to float to the top of an expression by moving the surrounding expression to surround the control operator's $K$ slot.
$$(\F^K_pv)\;e_2 \mapsto \F^{K\;e_2}_pv \qquad
  v_1\;\F^K_pv \mapsto \F^{v_1\;K}_pv$$
$$\#_{\F_p^Kv}^{e_h}e \mapsto \F_p^{\#_K^{e_h}e}v \qquad
  \#^{\F_p^Kv}_{p'}e \mapsto \F_p^{\#^K_{p'}e}v$$
$$\#_{p'}^{v_h}\F^K_pv \mapsto \F^{\#_{p'}^{v_h}K}_pv
  	\quad\textrm{where }p \neq p'$$
$$\F_{\F^K_pv}e \mapsto \F^{\F_Ke}_pv \qquad
  \F_{p_1}\F^K_{p_2}v \mapsto \F^{\F_{p_1}K}_{p_2}v$$
$$\#_\blacksquare^{\F_p^K v}e \mapsto \F_p^{\#_\blacksquare^K e}v$$

Although most of the calculus is confluent, the presence of $\nu$ and its associated reduction rule present some difficulties.
The difficulty occurs because some of the reduction rules make a copy of an expression.
Since $\nu$ results in a distinct prompt on each evaluation, we will note a difference whether the $\nu$ expression is copied or its result is copied.
Since most language make use of a left-to-right applicative order evaluation strategy, we will simply generalize this strategy to our calculus.
The system is then trivially confluent, as there is at most one reduction possible.
No doubt conservative extensions of this strategy may be adopted, and we leave this open to later authors, who may wish to make additional extensions (such as the inclusion of mutable state) which would impact the extension.

When we have $e_1/P_1 \mapsto_R e_2/P_2$, then we also have
$$e_1\;e/P_1 \mapsto_R e_2\;e/P_2 \qquad
  v\;e_1/P_1 \mapsto_R v\;e_2/P_2$$
$$\#_{e_1}^{e_h}e/P_1 \mapsto_R \#_{e_2}^{e_h}e/P_2 \qquad
  \#_{p}^{e_1}e/P_1 \mapsto_R \#_{p}^{e_2}e/P_2 \qquad
  \#_{p}^{v_h}e_1/P_1 \mapsto_R \#_{p}^{v_h}e_2/P_2$$
$$\F_{e_1} e/P_1 \mapsto \F_{e_2} e/P_2 \qquad
  \F_{p} e_1/P_1 \mapsto \F_{p} e_2/P_2$$
$$\#_\Uparrow^{e_g}e_1/P_1 \mapsto \#_\Uparrow^{e_g}e_2/P_2 \qquad
  \#_\Downarrow^{e_g}e_1/P_1 \mapsto \#_\Downarrow^{e_g}e_2/P_2$$
$$\#_\blacksquare^{e_1}e/P_1 \mapsto \#_\blacksquare^{e_2}e/P_2 \qquad
  \#_\blacksquare^{v_h}e_1/P_1 \mapsto \#_\blacksquare^{v_h}e_2/P_2$$

We are finally ready to define program equivalence.
We say that $e_1 \equiv e_2$ if and only if there is an $e'$ for which $e_1 \mapsto^* e'$ and $e_2 \mapsto^* e'$, where $\mapsto^*$ is the reflexive-transitive closure of $\mapsto$.

Finally in this section, we will just gather the syntactic abbreviations we had defined in our stepwise development of $\lambda_{SFCC}$.
First, there are a few additional ways of writing functions.
$$\x_1,\ldots,x_n.e \equiv \x_1.\x_2\ldots\x_n.e$$
$$\lambda\_.e \equiv \x.e \quad\textrm{ where }x \notin fv(e)$$
$$\lambda().e \equiv \lambda\_.e \qquad
  e\;() \equiv e\;v$$
Then, we have sequencing and let-expressions.
$$e_1; e_2 \equiv (\lambda\_.e_2)\;e_1$$
$$\letin{x = e'}{e} \equiv (\x.e)\;e'$$
$$\letin{x_1 = e_1; \ldots; x_n = e_n}{e} \equiv \letin{x_1 = e_1}{\ldots\letin{x_n = e_n}{e}}$$
Since we will almost always write the $\F$ operator with an initially empty continuation, we have:
$$\F_{e_p}e \equiv \F_{e_p}^{\square}e$$
Finally, the extent guards for local control flow and for either local or non-local control flow area are not in the core grammar, since they are easily simulated.
$$\#_\uparrow^{e_g}e \equiv e_g; e \qquad
  \#_\downarrow^{e_g}e \equiv \letin{x=e}{e_g; x}$$
$$\#_\upuparrows^{e_g}e \equiv \#_\uparrow^{e_g}\#_\Uparrow^{e_g}e \qquad
  \#_\downdownarrows^{e_g}e \equiv \#_\downarrow^{e_g}\#_\Downarrow^{e_g}e$$





\maybePage
\section{Relationship to Existing Operators}
\label{sec:relationship}

\subsection{Preliminaries}
Before delving into various operators, we will first want to define a few control operators of our own to help us along the way.

Note that once the control operator $\F$ reaches its delimiter, values are passed to the control operator's body value.
Often, the body will be ill-equipped to continue computation, and will simply pass control directly to the delimiter's handler.
This we call \emph{abort} and write with the $\A$ operator:
\begin{align*}
\A_{e_p}e \equiv \letin{&p = e_p, x = e\\}%
                       {&\F_{p}\lambda h,k. h\;k\;x}
\end{align*}

We will also want to agree on a few prompts beforehand.
In several FCC structures, there is no notion of prompt, so for these we will introduce a default prompt, $\mathbf{p}$ for these structures to use.
We will also want a prompt $\mathbf{p_0}$ that is only meant to be installed at the top of the program, usually by the run-time system.
Such a prompt will allow us to implement control structures that do capture or discard the entire continuation.
We will introduce a few more prompts as we go along, but each of them will be smaller in scope.
Since we have written these in boldface, their appearance represents the prompt itself, rather than a name to which the prompt is bound; thus, we are able to ignore questions of variable capture.

These constructs, and those we will develop in the rest of this section will be useful again in \ref{sec:applications}.

\maybePage
\subsection{MFDC}

Since we have already mentioned the major inspirations for $\lambda_{SFCC}$, it only makes sense to begin our survey of relationships with those inspirations.
The system $\pmFpm$ given in \textit{A Monadic Framework for Delimited Continuations}\cite{MFDC}, which we abbreviate MFDC, relies on an extension of the lambda calculus with a system of four operators.

\begin{itemize}
\item Their operator $\mathsf{newPrompt}$ creates a fresh prompt.
\item Their delimiting operator is $\mathsf{pushPrompt}\;e_p\;e$.
It takes a prompt an an expression, but it does not install a handler.
\item Their control operator is $\mathsf{withSubCont}\;e_p\;e_h$.
It evaluates its two arguments, a prompt and a function.
It captures the appropriate continuation and passes it to the function.
\item Finally, their $\mathsf{pushSubCont}\;e_k\;e$ takes a subcontinuation value and adds it to the control stack before evaluating its body.
\end{itemize}

Two of these operators are directly reflected in $\lambda_{SFCC}$, and a third is easily translated:
\begin{align*}
\mathsf{newPrompt} &\equiv \nu \\
\mathsf{pushSubCont}\;e_k\;e &\equiv e_k\;e \\
\mathsf{withSubCont} &\equiv
    \lambda p,f. \F_{p}\lambda \_,k. f\;k \\
\end{align*}
The implementation of $\mathsf{withSubCont}$ does little more than arrange for the prompt-specified handler to be discarded during the call to the function.

The fourth operator requires a design choice.
We will implement an equivalence of the form $\mathsf{pushPrompt}\;e_p\;e \equiv \#_{e_p}^{v_?}e$, but the choice of $v_?$ is not fixed by the semantics of \cite{MFDC}.
Two major options present themselves:
either provide a simple value, not necessarily usable as a handler,
or provide a handler which will continue to propagate the process of continuation capture when it is called.
\begin{align*}
\mathsf{pushPrompt}\;e_p\;e &\equiv \#_{e_p}^{()}e \\
&\textrm{or} \\
\mathsf{pushPrompt}\;e_p\;e &\equiv
    \letin{p=e_p}\#_{p}^{\lambda k,x.\F_{p}\lambda h',k'.h'\;(k' \circ k)\;x}e \\
%    \letin{p=e_p}\#_{p}^{\lambda k,x.\F_{p}^{k\;\square}x}e \\
\end{align*}
Neither option is sufficient to rule out crashes and unexpected behavior when the operators of MFDC are used in $\lambda_{SFCC}$, but for the purposes of illustrating MFDC, the issue will never become important.
In the interests of brevity we will simply choose the former in our example reductions.

We will now examine an example from \cite{MFDC}.
\begin{align*}
&(\lambda p. 2 + \mathsf{pushPrompt}\;p \\
&\qquad \mathsf{if}\;(\mathsf{withSubCont}\;p \\&\qquad\quad (\lambda k. (\mathsf{pushSubCont}\;k\;\mathbf{False}) +
            (\mathsf{pushSubCont}\;k\;\mathbf{True}))) \\
&\qquad \mathsf{then}\;3\;\mathsf{else}\;4) \\&\mathsf{newPrompt} \\
\end{align*}
This translates into $\lambda_{SFCC}$ as the following (we have taken the liberty of applying an $\eta$-conversion in the body of the $\F$):
\begin{align*}
&(\lambda p. 2 + \#_p^{()} \\
&\qquad \mathsf{if}\;((\lambda x_p,x_f. \F_{x_p}\lambda \_.x_f)\;p \\&\qquad\quad \lambda k. (k\;\mathbf{False}) +
            (k\;\mathbf{True})) \\
&\qquad \mathsf{then}\;3\;\mathsf{else}\;4) \\&\nu \\
\end{align*}

First, the $\nu$ is evaluated to a fresh prompt---let's represent that with $\mathbf{a}$---which is then substituted into the body of the lambda.
Another pair of $\beta$-reductions bring the expression to the point where we will need to begin continuation capture.
\begin{align*}
2 + \#_\mathbf{a}^{()} &\mathsf{if}\;(\F_\mathbf{a}\lambda \_,k. \\
&\quad (k\;\mathbf{False}) + (k\;\mathbf{True}))\; \\
&\mathsf{then}\;3\;\mathsf{else}\;4 \\
\end{align*}

Continuation capture terminates quickly, since there is a delimiter just one node away.
The base-case reduction for continuation capture leads to the following, which reduces to $9$ by a few simple reductions:
\begin{align*}
& 2 +
	(\lambda \_,k. (k\;\mathbf{False}) + (k\;\mathbf{True}))
	\;()
	\;\angles{\mathsf{if}\;\square\;\mathsf{then}\;3\;\mathsf{else}\;4} \\
\mapsto{} & 2 +
	(\angles{\mathsf{if}\;\square\;\mathsf{then}\;3\;\mathsf{else}\;4}\;\mathbf{False})
	+ (\angles{\mathsf{if}\;\square\;\mathsf{then}\;3\;\mathsf{else}\;4}\;\mathbf{True}) \\
\mapsto{} & 2 + (\mathsf{if}\;\mathbf{False}\;\mathsf{then}\;3\;\mathsf{else}\;4) + (\mathsf{if}\;\mathbf{True}\;\mathsf{then}\;3\;\mathsf{else}\;4) \\
\mapsto{} & 2 + 4 + 3 \\
\mapsto{} & 9
\end{align*}


\maybePage
\subsection{fcontrol}

The next inspiration we mention is fcontrol.
Recall that fcontrol consists of the delimiting operator $\%$, and the control operator $\mathsf{fcontrol}$.
The delimiting operator requires a body expression and a handler; the control operator requires only an abort value.
Both operators may be equipped with a prompt, or allowed to use the default prompt, $\mathbf{p}$.

Implementations of these operators are given below.
Since the semantics in \cite{HandlingControl} applies the handler to arguments in reverse order, we will make use here of the well-known function $\mathbf{flip} = \lambda f,x,y. f\;y\;x$.
The following are faithful implementations of the operators defined in \cite{HandlingControl}:
\begin{align*}
\textbf{run-tagged} &\equiv \lambda p,f,h. \#_p^h f\;() \\
\textbf{fcontrol-tagged} &\equiv \lambda p,x. \A_p x \\
\textbf{run} &\equiv \textbf{run-tagged}\;\mathbf{p} \\
\mathbf{fcontrol} &\equiv \textbf{fcontrol-tagged}\;\mathbf{p} \\
(\%\;e\;e_h) &\equiv \textbf{run}\;(\lambda \_.e)\;e_h \\
\end{align*}

\maybePage
\subsection{Scope Statements and Dynamic Wind}

We will cover our last two inspirations together.
The implementations here justify our descriptions of extent guards in \ref{subsec:extentGuards}.

We mentioned that extent guards are a generalization of scope statements, where now execution on entrance is made available alongside execution on exit.
Here, we see that the three scope statements correspond to our three extent exit operators.
\begin{align*}
\textsf{scope(exit)}\;e_g; e &\equiv \#_\downdownarrows^{e_g}e \\
\textsf{scope(success)}\;e_g; e &\equiv \#_\downarrow^{e_g}e \\
\textsf{scope(failure)}\;e_g; e &\equiv \#_\Downarrow^{e_g}e \\
\end{align*}
Of course, $\lambda_{SFCC}$ also has three extent entrance operators which operate symmetrically.

We also mentioned that our extent guards split the dynamic wind operator into its component parts.
In Scheme, dynamic-wind is a function which takes three zero-argument functions, rather than being a special form taking three subexpressions; this is the reason for the enclosing lambda in our implementation.
We see that all six extent guards are involved in dynamic wind:
the $\#_\downdownarrows$ and $\#_\upuparrows$ provide the most concise implementation, but recall that these can be expressed in terms of $\#_\downarrow, \#_\Downarrow$ and $\#_\uparrow, \#_\Uparrow$ respectively.
\begin{align*}
\textbf{dynamic-wind} &\equiv \lambda pre,body,post.
    \#_\upuparrows^{pre\;()}\#_\downdownarrows^{post\;()}body\;() \\
  &\equiv \lambda pre,body,post.
    \#_\uparrow^{pre\;()}\#_\Uparrow^{pre\;()}
    \#_\downarrow^{post\;()}\#_\Downarrow^{post\;()}
    body\;()
\end{align*}


\maybePage
\subsection{Try-catch-finally}
\label{subsec:implExn}

We now move on to a demonstration that exception handling can be easily implemented within $\lambda_{SFCC}$.
This is the essence of our claim of the unification of exception handling and FCC.
Since each language has their own idiosyncratic take on exception handling, we will illustrate a few systems, at which point the reader should be prepared to implement their favorite exception system.
Although most languages offer try-catch constructs as statements, we will offer them as an expression so as not to complicate our grammar.

We begin with the components of the system which remain constant for all the systems we will examine.
We will need a prompt, which we will write $\mathbf{exn}$, specifically set aside for exceptions.
The throw operator simply needs to abort up to the nearest exception handler, which we will implement with $\A$.
The try-finally construct need only ensure that some code is executed no matter how control exits from a block, which is clearly implementable with $\#_\downdownarrows$.
\begin{align*}
\textsf{throw }e &\equiv \A_\textbf{exn}e \\
\textsf{try }e\textsf{ finally }e_f &\equiv
	\#_\downdownarrows^{e_f}e \\
\end{align*}

The first definition of try-catch is a particularly simple system, an untyped try-catch-finally, similar to that which appears in JavaScript.
Here, the try-catch implementation need only install an exception handler, which in turn need worry only about the value aborted with (the exception itself), and may discard the continuation.
\begin{align*}
\textsf{try }e_{body}\textsf{ catch }x:e_h &\equiv
	\#_\textbf{exn}^{\lambda \_,x.{e_h}}e_{body}
	\\
\end{align*}

We can also include stack traces in our simulation, provided that the language implementation has some facility, $\mathbf{addTrace}$ that will add a stack trace to an exception value.
We only need to change the handler to first add the trace before moving to the handler's body.
$$\textsf{try }e_{body}\textsf{ catch }x:e_h \equiv
	\#_\textbf{exn}^{\lambda k,a.{\letin{x=\mathbf{addTrace}\;k\;a}{e_h}}}e_{body}$$
We need only make the restriction that $k,a \notin fv(e_f)$ so as to avoid inadvertent variable capture.
Here again, the continuation value is not accessible to the user, so the implementation does not allow the continuation to be resumed.

Our next refinement allows a catch block to trigger only on certain subtypes of an exception type.
To do this, we will use the metavariable $\tau$ to represent an arbitrary type, and the expression $e\textsf{ instanceof }\tau$ to check at runtime whether the type of $e$ is a subtype of $\tau$.
We can implement this using either of our prior untyped try-catch definitions.
Now however, the handler first checks the type of the exception value before moving to the user's handling code.
When the type is not suitable, then the exception is thrown again, so that the exception has another chance to be caught by an acceptable handler.
\begin{align*}
\textsf{try }e_{body}\textsf{ except }\tau\;x:e \equiv{}
	&\textsf{try}\;e_{body}\;\textsf{ except }x: \\
	&\quad\textsf{if}\;x\textsf{ instaceof }\tau\\
	&\qquad\textsf{then}\;e_h \\
	&\qquad\textsf{else throw}\;x \\
\end{align*}

Finally, as a syntactic nicety, we allow for sequences of $\textsf{try}\ldots\textsf{try}$ to be compressed into a single \textsf{try} keyword.
Thus, instead of writing
$$\textsf{try try try }e_a\textsf{ catch }\tau_1\;x:e_b\textsf{ catch }\tau_2\;x:e_c\textsf{ finally }e_d$$
we can simply write the more familiar
$$\textsf{try }e_a\textsf{ catch }\tau_1\;x:e_b\textsf{ catch }\tau_2\;x:e_c\textsf{ finally }e_d$$
which we now recognize as being equivalent to

\begin{tabular}{l}
$\#_\downdownarrows^{e_d}$ \\
  $\quad\#_\mathbf{exn}^{\lambda \_,x.
  	\textsf{if }x\textsf{ isinstance }\tau_2
	\textsf{ then }e_c
	\textsf{ else }\A_\mathbf{exn}x}$ \\
  $\qquad\#_\mathbf{exn}^{\lambda \_,x.
  	\textsf{if }x\textsf{ isinstance }\tau_1
	\textsf{ then }e_b
	\textsf{ else }\A_\mathbf{exn}x}$ \\
  $\qquad\quad e_a$ \\
\end{tabular}

The apparatus we have now developed includes all the intuitions for how exception handling is supposed to work.
If no exception is thrown, the catch blocks are not executed, but the finally block is.
On the other hand, if an exception is thrown in a try block, the exception is matched against the catch clauses one-by-one in order.
Once the appropriate catch block has executed, the finally clause then executes.
If no catch block is suitable, then the exception continues to propagate to the next dynamically-enclosing try-catch block, but not before the finally block is executed.

Exception handling was developed independently of both delimited control and formal methods, and so we did not expect the translation to be as small and straightforward as our definitions related to other FCC constructs.
Nevertheless, each translation we give is very simple.
%More importantly, the translations operate on the source program in local sections rather performing a transformation on the program as a whole.
This demonstration makes clear two important points.
First, it shows that our system is capable of handling concepts broader than its own historical influences, and is therefore quite general.
Second, as exception handling is present in nearly every production programing language, it shows that our system is capable of solving real-world issues that commonly appear in production programs.




%\begin{figure}[H]
%\caption{An Average Exception System}
%\label{exceptionTranslation}
%
%\begin{tabular}{rcl}
%$\textsf{throw}\;e$ & $\equiv$ &
%\begin{tabular}{l}
%	$\textsf{capture}[\textbf{exn}]\;\lambda trace.$ \\
%	$\;\; \abort{\textbf{exn}}{\textbf{add-stack-trace}\;e\;trace}$ \\
%\end{tabular}
%\end{tabular}
%
%\vspace{1em}
%
%\begin{tabular}{rcl}
%\begin{tabular}{l}
%	$\textsf{try}\;e$ \\
%	$\textsf{catch}(\sigma_1\;x_1)\;e_1$ \\
%	$\ldots$ \\
%	$\textsf{catch}(\sigma_n\;x_n)\;e_n$ \\
%	$\textsf{finally}\;e_f$ \\
%\end{tabular}	
%& $\equiv$ &
%\begin{tabular}{l}
%	$\textsf{onExit}\;e_f;$ \\
%	$\textsf{handle}[\textbf{exn}]\;(\lambda x.$ \\
%	$\;\; \textsf{if}\;x\;\textsf{instanceof}\;\sigma_1$ \\
%	$\;\;\;\; \textsf{then}\;\textsf{let}\;x_1=x\;\textsf{in}\;e_1$ \\
%	$\;\; \ldots$ \\
%	$\;\; \textsf{elif}\;x\;\textsf{instanceof}\;\sigma_n$ \\
%	$\;\;\;\; \textsf{then}\;\textsf{let}\;x_n=x\;\textsf{in}\;e_n$ \\
%	$\;\; \textsf{else}\;\textsf{throw}\;x$) \\
%	$\textsf{in}\;e$ \\
%\end{tabular}
%\end{tabular}
%
%\end{figure}

\maybePage
\subsection{Miscellaneous}
\label{subsec:implMisc}

We will now rapidly review a number of control operators we mentioned in \S\ref{delimControl}.
We there noted a resemblance between $^-\F^-$ and cupto, which we now implement:
\begin{align*}
\#e &\equiv \#_\mathbf{p}^{()}e \\
^-\F^-e &\equiv
	\letin{f=e}{\F_\mathbf{p}\lambda \_,k.f\;k} \\
%
\textsf{set }e_p\textsf{ in }e &\equiv
	\#_{e_p}^{()}e \\
\textsf{cupto }e_p\textsf{ as }k\textsf{ in }e &\equiv
	\letin{p = e_p,f=e}{\F_p\lambda \_,k.f\;k} \\
\end{align*}

In light of this similarity, we can simply rename cupto to $^-\F^-$ with a subscript to specify the prompt.
We can also do the same for \textsf{set}.
\begin{align*}
\#_{e_p}e &\equiv \#_{e_p}^{()}e \\
	&\equiv \textsf{set }e_p\textsf{ in }e \\
^-\F^-_{e_p}e &\equiv
	\letin{p = e_p,f=e}{\F_{p}\lambda \_,k.f\;k} \\
	&\equiv \letin{p=e_p,f=e}{\textsf{cupto }p\textsf{ as }k\textsf{ in }f\;k} \\
\end{align*}

We now have three more variations on $\F$ with subscript, where each varies based on whether they reinstall the prompt and when.
\begin{align*}
^+\F^-_{e_p}e &\equiv
	\letin{p=e_p,f=e}{^-\F_{p}^-\lambda h,k.\#_{p}^hf\;k} \\
^-\F^+_{e_p}e &\equiv
	\letin{p=e_p,f=e}{^-\F_{p}^-\lambda h,k.f\;\angles{\#_{p}^hk\;\square}} \\
^+\F^+_{e_p}e &\equiv
	\letin{p=e_p,f=e}{^-\F_{p}^-\lambda h,k.\#_{p}^hf\;\angles{\#_{p}k\;\square}} \\
\end{align*}
With these, we can give the implementation of the entire $\pmFpm$ system quite easily by noting 
\begin{align*}
\#e &\equiv \#_\mathbf{p}e \\
\pmFpm e &\equiv{} \pmFpm_\mathbf{p}e
\end{align*}

With just these few identifications, we have implemented every control operator discussed in \S\ref{delimControl} short of fcontrol, which was discussed earlier.


It was noted in \cite{MFDC} that the control operator could be factored into two operators: one that captured the continuation and one which performed abort.
It was also claimed that two systems differing only on this choice of factoring were equivalent.
We will briefly show this here.
As with our implementation of the MFDC operators, we will not expend much effort on the handlers installed with prompts, since these do not appear in \cite{MFDC}.

In the definition of capture, we arrange for the prompt and continuation to be re-installed before moving to use the continuation in the body of the operator.
Aborting is even simpler: we simply discard the continuation and move directly to the body.
To get back to MFDC's \textsf{withSubCont} operator, we simply chain these two new control structures together.
\begin{align*}
\textbf{capture} &\equiv
	\lambda p,f.\F_p\lambda h,k.\#_p^hk\;(f\;k) \\
\textbf{abort } &\equiv
	\lambda p.\angles{\F_p\lambda \_,\_.\square} \\
\textbf{withSubCont} &\equiv
	\lambda p,f.\textbf{capture}\;p\;(\lambda k. \\
	&\qquad\qquad\;\;	\textbf{abort }p\;(f\;k)) \\
\end{align*}



%In \S\ref{undelimControl}, we claimed that a system of undelimited control is easily simulated by a system of delimited control.




%\maybePage
%\section{Design of a Source Language}
%
%disallow anything other than $\square$ as $K$ in $\F$; always raise a stock exception when a barrier is tripped; a default handler for operators that do not normally specify handlers; only allow $\#_\upuparrows$ instead of independent $\#_\uparrow$ and $\#_\Uparrow$.

\maybePage
\section{Efficient Implementation}
\label{sec:efficientImpl}

Now that we have developed our semantics, we turn to efficiency concerns.
We will show that $\lambda_{SFCC}$ need not impose inherent algorithmic complexity penalties.
In particular, if FCC techniques are not used, then the interpreter will be no slower than a CESK machine: all the cost of FCC is limited to capturing and restoring continuations.
To demonstrate this, we develop a simple interpreter for the language.

\maybePage
\subsection{Supporting Definitions}

We will use largely the same grammar as in our semantics, with the exception of the representation of continuations.
We apply a zipper transformation to the data structure; here continuations are represented as a list of size 1 continuations, which we call contexts.
We continue our upward-growing stack convention in text, and also adopt a leftward-growing convention.
Further, a continuation begin captured will not be stored by the control operator, but in a separate control stack.
We therefore eliminate the superscript from the $\F$ operator.

\begin{figure}[H]
\caption{Interpreter Grammar}
\label{fig:startGrammar}

\begin{tabular}{llcl}
Values & $v$ & ::= & $\lambda x.e \mid K \mid p$ \\
Expressions & $e$ & ::= & $v \mid x \mid e\;e$ \\
& & $|$ & $\nu \mid \#_{e_p}^{e_h}e \mid \F_{e_p}e$ \\
& & $|$ & $\#_\Uparrow^{e_g}e
    \mid   \#_\Downarrow^{e_g}e
    \mid   \#_\blacksquare^{e_h}e$ \\
Continuations & $K$ & ::= & $[k_n, \ldots, k_1]$ \\
Contexts & $k$ &
     ::= & $\square\;e
     \mid  v\;\square$ \\
& & $|$ & $\#_\square^{e_h} e
    \mid   \#_p^\square e
    \mid   \#_p^v \square$ \\
& & $|$ & $\F_\square e
    \mid   \F_p \square$ \\
& & $|$ & $\#_\Uparrow^{e_g}\square
    \mid   \#_\Downarrow^{e_g}\square
    \mid   \#_\blacksquare^\square e
    \mid   \#_\blacksquare^e\square$ \\
\end{tabular}
\end{figure}



%We should note an interesting possible feature of boundaries, which is to attach an alternate expression to a boundary.
%When control attempts to escape from a sub-interpreter bounded by one of these boundaries, the stack is truncated through the boundary and evaluation resumes with alternate expression.
%For example, a language with primitive $\mathbf{eval}$ might insert a boundary before interpreting the passed expression such that an attempt to escape from the sub-interpreter instead raises a predictable exception in the main interpreter.
%This strategy is a necessary part of sandboxing under FCC; with it we can ensure that sandboxed code cannot take control over trusted code.\cite{addDelimControlProduction}
%Though important, this feature is a straightforward extension and tangential to our purposes, so we will only consider boundaries without alternatives.


We can adjust the top of the stack through concatenation; we write $K'K$ to mean the extension of the continuation $K$ with an the contexts of $K'$ on top.

We will need two functions, $enterGuard$ and $exitGuard$, of two parameters: a continuation and a base-case expression.
Intuitively, what these guards do is to accumulate a sequence of expressions from the extent guards, $\#_\Uparrow$ and $\#_\Downarrow$ respectively, in the continuation.
The precise sequence is different for the two functions: the result of $enterGuard$ will evaluate the guards in FIFO order, but $exitGuard$ will evaluate them in LIFO order with respect to when the corresponding extent guards were pushed to the stack.
Either way, the last expression in the sequence is the base-case expression.
Thus, these functions formalize the process of collecting and evaluating control guards that were registered by a guard expression before continuing on.
Formally, we have
\begin{itemize}
\item $enterGuard(K, e) = e_g;enterGuard(K^\uparrow, e)$ when $K = K^\uparrow[\#_\Uparrow^{e_g}\square]K^\downarrow$ and $enterGuard(K^\downarrow, e) = e$,
\item or $enterGuard(K, e) = e$ otherwise.
\item $exitGuard(K, e) = e_g;exitGuard(K^\downarrow, e)$ when $K = K^\uparrow[\#_\Downarrow^{e_g}\square]K^\downarrow$ and $exitGuard(K^\uparrow, e) = e$,
\item or $exitGuard(K, e) = e$ otherwise.
\end{itemize}


We also require the partial function $splitStack$ taking a continuation and a prompt.
It finds in the passed continuation the location of the most-recently install control delimiter either of the passed prompt or the barrier.
It then returns the stack portions strictly above and below the delimiter, as well as the handler attached to that delimiter.
The result is tagged with whether control was delimited by a delimiter for the prompt or by a barrier.
%However, $splitStack$ will not allow the upper portion to contain a barrier; this protects the rest of the process from non-local control-flow bugs in any ostensibly control-pure expression.
Formally, we have $splitStack(K, p)$ defined as
\begin{itemize}
\item either $\#\angles{K^\uparrow, v_h, K^\downarrow}$
      when $K = K^\uparrow[\#_p^{v_h}\square]K^\downarrow$
\item or else $\blacksquare\angles{K^\uparrow, v_h, K^\downarrow}$
      when $K = K^\uparrow[\#_\blacksquare^{v_h}\square]K^\downarrow$,
\item provided both $K^\uparrow \neq \hat{K}^\uparrow[\#_p^{\hat{v}_h}\square]\hat{K}^\downarrow$
\item and $K^\uparrow \neq \hat{K}^\uparrow[\#_\blacksquare^{\hat{v}_h}\square]\hat{K}^\downarrow$
\end{itemize}
In all other cases, the function is undefined.


\maybePage
\subsection{Evaluation}

Our interpreter is similar to a SESK machine, except that we have no need for an environment (we will be performing substitution directly), and the only store necessary is a pool of fresh prompts.
We define evaluation of an expression using a transition system operating on triples of the form $\angles{e,K,P}$, where $P$ is a set of prompts.
The translation relation is written $\longmapsto$, and its reflexive transitive closure as $\longmapsto^*$.
We say that $eval(e) = v$ when $\angles{e,[],P} \longmapsto^* \tuple{v,[],P'}$.
In all other cases, $eval(e)$ is undefined.

Though there appear to be many transitions, there are really only a few ideas present which determine the pattern.
We've split our reduction rules into three groups based on common theme.
The reductions in Figure~\ref{fig:redexSearch} simply evaluate subexpressions according to the order of evaluation.
Those in Figure~\ref{fig:remove-stack-marks} unwind values past the various stack marks which can appear in the continuation.
The third group, in Figure~\ref{fig:interpreter-core}, performs the interesting work of the interpreter;
we will discuss each of these rules individually.


\begin{figure}[H]
\caption{Searching for a Redex}
\label{fig:redexSearch}
\begin{tabular}{rrcl}
$\textsc{app}_1$: &
	$\angles{e\;e', K,P}$ & $\longmapsto$ &
	$\angles{e,[\square\;e']K,P}$ \\
$\textsc{app}_2$: &
	$\angles{v, [\square\;e]K,P}$ & $\longmapsto$ &
	$\angles{e,[v\;\square]K,P}\quad$ where $v \neq K'$ \\
$\textsc{\#}_1$: &
	$\angles{\#_{e_p}^{e_h}e, K, P}$ & $\longmapsto$ &
	$\angles{e_p,[\#_{\square}^{e_h}e]K, P}$ \\
$\textsc{\#}_2$: &
	$\angles{v_p, [\#_{\square}^{e_h}e]K, P}$ & $\longmapsto$ &
	$\angles{e_h,[\#_{v_p}^{\square}e]K, P}$ \\
$\textsc{\#}_3$: &
	$\angles{v_h, [\#_{v_p}^{\square}e]K, P}$ & $\longmapsto$ &
	$\angles{e,[\#_{v_p}^{v_h}\square]K, P}$ \\
$\textsc{c}_1$: &
	$\angles{\F_{e_p}e, K, P}$ & $\longmapsto$ &
	$\angles{e_p, [\F_\square e]K,P}$ \\
$\textsc{c}_2$: &
	$\angles{v, [\F_\square e]K, P}$ & $\longmapsto$ &
	$\angles{e, [\F_v \square]K, P}$ \\
$\Uparrow_1$: &
	$\angles{\#_\Uparrow^{e_g}e, K, P}$ & $\longmapsto$ &
	$\angles{e,[\#_\Uparrow^{e_g}\square]K, P}$ \\
$\Downarrow_1$: &
	$\angles{\#_\Downarrow^{e_g}e, K, P}$ & $\longmapsto$ &
	$\angles{e,[\#_\Downarrow^{e_g}\square]K, P}$ \\
$\blacksquare_1$: &
	$\angles{\#_\blacksquare^{e_h}e, K, P}$ & $\longmapsto$ &
	$\angles{e_h,[\#_\blacksquare^{\square}e]K, P}$ \\
$\blacksquare_2$: &
	$\angles{v_h, [\#_\blacksquare^{\square}e]K, P}$ & $\longmapsto$ &
	$\angles{e,[\#_\blacksquare^{v_h}\square]K, P}$ \\
\end{tabular}
\end{figure}

\begin{figure}[H]
\caption{Removing Stack Marks}
\label{fig:remove-stack-marks}
\begin{tabular}{rrcl}
$\#$: &
	$\angles{v, [\#_p^{v_h}\square]K, P}$ & $\longmapsto$ &
	$\angles{v, K, P}$ \\
$\Uparrow$: &
	$\angles{v, [\#_\Uparrow^{v_h}\square]K, P}$ & $\longmapsto$ &
	$\angles{v, K, P}$ \\
$\Downarrow$: &
	$\angles{v, [\#_\Downarrow^{v_h}\square]K, P}$ & $\longmapsto$ &
	$\angles{v, K, P}$ \\
$\blacksquare$: &
	$\angles{v, [\#_\blacksquare^{v_h}\square]K, P}$ & $\longmapsto$ &
	$\angles{v, K, P}$ \\
\end{tabular}
\end{figure}

\begin{figure}[H]
\caption{Core Reductions}
\label{fig:interpreter-core}
\begin{tabular}{rrcl}
$\textsc{app}_\lambda$: &
	$\angles{v, [(\lambda x.e)\;\square]K,P}$ & $\longmapsto$ &
	$\angles{e[x\backslash v],K,P}$ \\
$\textsc{app}_K$: &
	$\angles{K', [\square\;e]K,P}$ & $\longmapsto$ &
	$\angles{e_g,K'K,P}$ \\
	&& where & $e_g = enterGuard(K',e)$ \\
$\textsc{new}$: &
	$\angles{\nu, K, \{p\} \cup P}$ & $\longmapsto$ &
	$\angles{p,K,P}$ \\
$\textsc{c}_\#$: &
	$\angles{v, [\F_p\square]K, P}$ & $\longmapsto$ &
	$\angles{e_g, K^\downarrow,P}$ \\
	&& where & $\#\angles{K^\uparrow, v_h, K^\downarrow} = splitStack(K,p)$ \\
	&& and & $e_g = exitGuard(K^\uparrow,v\;v_h\;K^\uparrow)$ \\
$\textsc{c}_\blacksquare$: &
	$\angles{v, [\F_p\square]K, P}$ & $\longmapsto$ &
	$\angles{e_g, K^\downarrow,P}$ \\
	&& where & $\blacksquare\angles{K^\uparrow, v_h, K^\downarrow} = splitStack(K,p)$ \\
	&& and & $e_g = exitGuard(K^\uparrow, v_h\;p\;K^\uparrow\;v)$ \\
\end{tabular}
\end{figure}



The introduction of new prompts is handled by the $\textsc{new}$ rule.
This is the only rule which modifies the supply of prompts.
In it, a prompt is removed from the supply and placed into the machine's active expression.
Once the programmer has a prompt value on the stack, it will propagate through the continuation, being manipulated, passed it to functions, or directly to other control operators.

Application is handled by the two rules $\textsc{app}_\lambda$, which comes directly from the lambda calculus, and $\textsc{app}_K$, which is responsible for applying continuations.
The later rule is the only place where the continuation might be extended with more than one context, and so this is the only place we see the use of the $enterGuard$ function.
After invoking $\textsc{app}_K$, the next expression to be evaluated includes not only the argument expression, but also the sequence of all $\#_\Uparrow$ extent guards in the applied continuation.

Finally, control capture is handled by the two rules $\textsc{c}_\#$ and $\textsc{c}_\blacksquare$;
they differ only in that the former is applied when the continuation is delimited by the appropriate prompt, and the later only when the continuation is delimited by a barrier.
These are the only two places where the continuation stack can be reduced in size by more than one context, so these are the only places we use the $exitGuard$ function.
Similar to the $\textsc{app}_K$ rule, the next expression to be evaluated includes a sequence of expressions drawn from extent guards, this time from the $\#_\Downarrow$ guards of the captured continuation.

There are a few places where the interpreter could get stuck.
In an application, if the first expression does not evaluate to a function or continuation, then evaluation cannot continue; this is as normal in the lambda calculus.
Further, if $splitStack$ is undefined in either $\textsc{c}_\#$ or $\textsc{c}_\blacksquare$, the interpreter will get stuck.
This situation represents an attempt to capture a continuation which is not delimited anywhere in the continuation.
This decision prevents unintentional undelimited control capture.

This particular interpreter was presented with simplicity in mind, so that the $splitStack$ function will take linear time on the size of the stack.
However, we can use metacontinuation technique as in \cite{MFDC} (FIXME I'm sure they got it from somewhere else) to reduce the cost to linear time in the number of delimiters in the stack.
This requires a slight transformation only on the internal representation of the control stack, which is why we did not involve those complications in our formalism.

\maybePage
\section{Applications}
\label{sec:applications}

We have previously given a few examples of control structures that can be implemented in $\lambda_{SFCC}$, but with the exception of try-catch-finally, these were implementations only of other low-level control constructs.
In this section, we will examine a wide range of higher-level control constructs which could see immediate use in programming.
We will also examine those control structures found in our literature review which have not already been implemented.

Inventory of control structures:
\begin{itemize}
\item\ [DONE] abort
\item\ [DONE] fcontrol
\item\ [DONE] MFDC
\item\ [DONE] scope statements
\item\ [DONE] dynamic-wind
\item\ [DONE] try/catch/else/finally: typed and untyped, with/without stack trace; with/without rethrow
\item\ [DONE] cupto
\item\ [DONE] $\pmFpm$
\item\ [DONE] capture (without abort)
%
\item\ [?] spawn
\item\ [\^{}] call/cc
\item\ [?] marker
\item\ [?] panic
%
\item\ [DONE] context manager
\item\ [DONE] labeled constructs
\item\ [.] zahn
\item\ [DONE] conjunction
\item\ [DONE] anaphoric if
\item\ [DONE] fluid variables
\item\ [DONE] subroutines
\item\ [-] coroutines
\item\ [DONE] stream
\item\ [.] amb
\item\ [.] concurrent algorithms (cooperating threads)
\end{itemize}


\maybePage
\subsection{Context Statement}

We already discussed context statements in \S\ref{context statements}, which is a technique in a few object-oriented languages.
An object, the context manager is equipped with methods for setup and clean up, which are executed whenever control enters and exits, respectively, the body of the statement.
The implementation presents itself immediately from this description.
We write $e.x(e'\ldots)$ to call the $x$ method of $e$ with the arguments $e'\ldots$, as usual.
%$$\textsf{with }e\textsf{ as }x\textsf{ in }e_{body}
%\equiv
%	\letin{x = e}{\#_\uparrow^{x.enter()}\#_\downdownarrows^{x.exit()}e_{body}}$$
$$\textsf{with }e\textsf{ as }x\textsf{ in }e_{body}
\equiv
	\letin{x = e}{\#_\upuparrows^{x.enter()}\#_\downdownarrows^{x.exit()}e_{body}}$$

%\subsection{Software Transactions}
%because they will look a lot like context managers
%
%$$\textsf{perform }e_{transact}; e_{rest} \equiv
%	\letin{x = e_{transact}}{
%	\#_\downarrow^{x.commit()}
%	\#_\Downarrow^{x.rollback()}e_{rest}}$$

\maybePage
\subsection{Fluid Variables}

Fluid variables are variables whose stored values can temporarily take on alternate values.
That is, if $e_f$ is a fluid variable, then in $\textsf{fluid }e_f := e_v; e$, that variable takes the value of $e_v$ within $e$, but outside takes its original value.

The simplest way to implement this behavior requires mutable reference cells.
If we have a mutable cell, $c$, then we use the notation $^*c$ to dereference the cell, obtaining its contents, and we use the notation $c := v'$ to store a new value, $v'$ into the cell.

Our implementation simply saves the old value of the fluid variable locally, then places appropriate extent guards around the body expression, $e$.
Whenever control enters the body expression, the new value is stored, and whenever control exits, the old value is reinstated.
\begin{align*}
\textsf{fluid}\;e_f:=e_v;e \equiv{}
	&\letin{c = e_f, \\
	&\quad\;\, x_0 = {}^*c, x = e_v \\
	&}{
	\#_\downdownarrows^{c\;:=\;x_0}\#_\upuparrows^{c\;:=\;x}}e
\end{align*}

\maybePage
\subsection{Early Exit}

One difference between functions in imperative and functional languages is that there is no early return from a fucntion in a functional language.
Those functions which allow early return we will call subroutines.
We can quite easily implement a ``return statement'' in a functional language with FCC.
For each subroutine, we create a prompt and install it around the subroutine with a handler that simply returns the value aborted with.
Finally, we make the name \texttt{return} available inside the body, which will abort up to the installed prompt with whatever value it is passed.
\begin{align*}
\textsf{subroutine }e \equiv{}
	&\letin{p=\nu, \\
	&\quad\;\, \texttt{return}=\x.\A_px \\
	&}{
	\#_p^{\lambda \_,x.x}e}
\end{align*}



After implementing early return from  subroutine, we can also implement more general early exit, similar to early exit from labeled loop constructs.
In this implementation, the label's name becomes available in the scope of the body expression as an abort function.
The abort is delimited by an installed handler which, just as with subroutines, which will simply arrange for the abort value to be returned unaltered.
\begin{align*}
l\!: \{e\} &\equiv \letin{p = \nu, l = \x.\A_px}{\#_p^{\lambda \_,x.x}e} \\
\end{align*}

The advantage of this construct over the labeled loop constructs of existing languages is that any expression may now be labeled and exited early from, rather than only those expressions and statements the language designer was able to predict the use of and encode into the language.
For example, with a labeled exit construct, it is simple to implement subroutines as above:
$$\textsf{subroutine }e \equiv \texttt{return}\!:\{e\}$$



%TODO zahn's construct.


An additional feature of these implementations is that the ability to perform early exit can be delegated to further functions.
We used this technique in \S\ref{subsec:intro-to-fcc} to implement foldl/ee.
We will now give an implementation and reduction sketch of that example in $\lambda_{SFCC}$.

\begin{align*}
\textbf{foldl/ee} &\equiv
	\lambda f, xs, z.\,break\!: \{foldl\;(f\;break)\;xs\;z\} \\
\textbf{all} &\equiv
	\lambda pred,xs.\, \letin{test =
		\lambda break,x,z.\,\textsf{if }pred\;x
		\textsf{ then }z\textsf{ else }break\;\textbf{false}
	\\&\qquad\qquad\quad\;\;}{\textbf{foldl/ee}\;test\;xs\;\textbf{true}} \\
\end{align*}

Notice in the reduction sketch how the $break$ function is passed to another function.
In particular, it is passed on to the reducing function, so that the $test$ function in $\textbf{all}$ can manipulate control flow as it needs.
We have chosen this example precisely because it shows that the ability to abort can be delegated to a function which was not even anticipated when we implemented \textbf{foldl/ee}.
This demonstrates the great potential for reuse of first-class control structures.

We now give a short reduction sketch of foldl/ee in $\lambda_{SFCC}$:
\begin{align*}
&\textbf{all}\;(\x.x>0)\;[0, 1] \\
%
&\letin{test =
	\lambda break,x,z.\,\textsf{if }(\x.x>0)\;x
	\textsf{ then }z\textsf{ else }break\;\textbf{false}
\\&\qquad}{\textbf{foldl/ee}\;test\;[0,1]\;\textbf{true}} \\
%
&break\!: \{foldl\;(\textbf{test}\;break)\;[0,1]\;\textbf{true}\} \\
%
&\#_\textbf{p}^{\lambda\_,x.x}
	foldl\;(\textbf{test}\;(\x.\A_\textbf{p}x))\;[0,1]\;\textbf{true} \\
%
&\#_\textbf{p}^{\lambda\_,x.x}
	foldl\;(\x,z.\,\textsf{if }(\x.x>0)\;x
	\textsf{ then }z\textsf{ else }(\x.\A_\textbf{p}x)\;\textbf{false})\;[0,1]\;\textbf{true} \\
%
&\#_\textbf{p}^{\lambda\_,x.x}
	foldl\;(\x,z.\,\textsf{if }(\x.x>0)\;x
	\textsf{ then }z\textsf{ else }(\x.\A_\textbf{p}x)\;\textbf{false})\;[1] \\
		&\qquad\qquad(\textsf{if }0>0\textsf{ then }\textbf{true}\textsf{ else }(\x.\A_\textbf{p}x)\;\textbf{false}) \\
%
&\#_\textbf{p}^{\lambda\_,x.x}
	foldl\;(\x,z.\,\textsf{if }(\x.x>0)\;x
	\textsf{ then }z\textsf{ else }(\x.\A_\textbf{p}x)\;\textbf{false})\;[1] \\
		&\qquad\qquad(\F_\textbf{p}\lambda h,k.\,h\;k\;\textbf{false}) \\
%
&\#_\textbf{p}^{\lambda\_,x.x}\F_\textbf{p}^K\lambda h,k.\,h\;k\;\textbf{false} \\
%
&(\lambda h,k.\,h\;k\;\textbf{false})\;(\lambda\_,x.x)\;\angles K \\
%
&(\lambda\_,x.x)\;\angles K\;\textbf{false} \\
%
&\textbf{false} \\
\end{align*}


%\maybePage
%\subsection{Coroutines}
%
%\begin{align*}
%\textsf{coroutine}\;e \equiv{}
%&\textsf{let }p = \nu, \\
%  &\quad\; k_0 = \angles{
%    \square; \\
%    &\qquad\qquad \letin{\texttt{yield}=\x.{\A_px}}{e}; \\
%    &\qquad\qquad \textsf{throw }\texttt{"stop iteration"}} \\
%  &\quad\; c = \nu_C\;k_0 \\
%  &\textsf{in }
%  \x.\#_p^{\lambda k,v.\,c:=k;\,v}\: ^*\!c\;x \\
%\end{align*}
%
%once you have coroutines, cooperating threads are easy to implement,
%and so is the nondeterminism monad

\maybePage
\subsection{Streams and Nondeterministic Choice}

A stream is a list structure whose values are computed on demand.\cite{SICP}
We will think of our streams here primarily as functions which can return multiple times.
Our introduction form for streams takes an expression and make \texttt{yield} available within the expression.
A call to \texttt{yield} produces the next item in the stream.
We access the elements of a stream by applying \textbf{uncons}, which results either in a cons cell, $(head:tail)$, where $head$ is the next element in the stream, and $tail$ is the rest of the stream, or else the special value \textbf{nil}, when the end of the stream is reached.

In our implementation, we create a new prompt, just as in our subroutines.
The yield operator takes the role of the return operator.
This time, the handler is used to return the yielded value along with the rest of the stream as a continuation.
The rest of the stream must be delimited again, with the same handler, so we use the Y-combinator to allow the hander to recursively install itself.

\begin{align*}
\textsf{stream}\;e &\equiv{}
\textsf{let }p = \nu, \\
  &\qquad\;\; h = \textbf Y\;\lambda h,k,x.(x:\#_p^hk) \\
  &\quad\;\,\textsf{in }
  \angles{\#_p^{h}\square; \letin{\texttt{yield}=\x.{\A_px}}{e}; \textbf{nil}} \\
\textbf{uncons} &\equiv \lambda s.\,s\;() \\
\end{align*}

Now that we have streams, it is simple to implement the \emph{amb} operator, which performs non-deterministic choice.
Essentially, the \textbf{amb} operator accepts a list of options, then chooses just the correct elements to ensure the following computation does not fail.

Of course, the technique cannot be magic, but it does rely on the backtracking ability of streams.
First, \textbf{amb} turns a list into a stream by yielding each element insuccession.
Then, we have two functions, \textbf{succeed} and \textbf{fail} which signal the corresponding situations to the implementation.
Finally, we use the $>\!>=$ operator to combine computations using \textbf{amb}.
What it does is to accumulate all the selections that succeeded (in general, a single \textbf{amb} computation might itself produce multiple possible results).
\begin{align*}
\textbf{amb} &\equiv \x s. \textbf{map}\;\texttt{yield}\;xs \\
\textbf{succeed} &\equiv \x. [x] \\
\textbf{fail} &\equiv [] \\
>\!>= &\equiv \lambda x,k. \textbf{fold}\;\textbf{concat}\;(\textbf{map}\;k\;x)\;[] \\
\end{align*}

As an example, we can use this technique to generate a list of all pairs of factors for a given number.
We simply select a number up to the input and another up to the first, and fail whenever the pair does not multiply to produce the input.
$$\textbf{factors} = \lambda a. [1..a] >\!>= \x.
                             [1..x] >\!>= \lambda y.
                             \textsf{if }x*y == a
                             \textsf{ then }\textbf{succeed}\;(x, y)
                             \textsf{ else }\textbf{fail}$$


\maybePage
\subsection{Subinterpreters}

In some cases it is useful to isolate the execution of some piece of code from the main process, e.g. to run third-party code, as in a REPL.
It is possible to call on the operating system to start a new process which runs the code, but this is usually overkill if the untrusted code does not run concurrently.
Instead, we could execute it in the same process by invoking a subinterpreter.

In the presence of FCC, care must be taken to ensure the code in the subinterpreter cannot capture a continuation which includes some of the main interpreter.
To do this, we can implement $eval$ using the $\#_\blacksquare$ delimiter, since it is able to intercept control operators of any prompt.
Indeed, the main purpose of the $\#_\blacksquare$ is to isolate any control operators within its dynamic extent from affecting the computational context above it.
The following implements a simple isolated subinterpreter which treats undelimited control within its argument as an error.
$$\textsf{subinterpreter}\;e = \#_\blacksquare^{\lambda\_,\_,\_.\textsf{throw}\;\texttt{"undelimited control flow"}}e$$



\maybePage
\subsection{Natural Language Constructs}

As our last examples in this section, we turn to a pair of constructs which, although they are quite natural in human language, have largely resisted integration into programming languages.
It turns out that both constructs are quite simple to encode with continuations.

The first is the ``anaphoric if''.
This corresponds to the constructs of the form ``if the chair next to the table is green, then repaint it''.
The salient features are that some value is obtained by a complex expression, and this same value is both passed to a predicate, and used in one or both branches of the if statement.
In most languages, this has to be laboriously encoded:
\begin{verbatim}
{
  let it = X;
  if (P) {A} else {B}
}
\end{verbatim}
where \texttt{it} is free in \texttt{V}, \texttt{A} and/or \texttt{B}.

With $\lambda_{SFCC}$, we can implement this pattern by temporarily suspending a continuation while we perform binding.
We create a new prompt and matching abort function, \texttt{break} as in our early exit operators.
We will obtain a value, bound to \texttt{it}, and a continuation by evaluating the predicate, $e_p$.
If $e_p$ evaluates to a call to \texttt{break}, then the tuple will be the aborted value and its continuation, but if no such call occurs, the value of the entire $e_p$ expression will be bound to a default, the empty continuation.
Once we have the \texttt{it}-value and a (possibly empty) continuation, we simply evaluate the rest of the continuation and branch on the result.
Scope rules ensure that \texttt{it} is available inside both the consequent, $e_c$, and alternate, $e_a$, expressions.
\begin{align*}
\textsf{ifitis }e_p\textsf{ then }e_c\textsf{ else }e_a \equiv{}
	&\letin{(\texttt{it}, k) = \letin{p = \nu, \texttt{break} = \x.\A_px
	\\&\qquad\qquad\quad\;\;}{\#_p^{\lambda k,x.(x,k)}(e_p, \angles{\square})}
	\\&}{\textsf{if }k\;\texttt{it}\text{ then }e_c\textsf{ else }e_a}
\end{align*}

Another common natural language construct is the distribution of a predicate over several objects, as in ``is $x$ equal to 3 or 4?'' and ``are both $x$ and $y$ positive?''.
This construction can become useful when programming;
indeed, a common Python idiom uses set inclusion to implement the former: \texttt{if x in \{3, 4\}}.
Unfortunately, set membership is only one instance of the pattern we find here.

In \cite{ContInNatlang} however, the author suggests that a natural interpretation of these constructs is in terms of continuations.
In this implementation, we will use a preselected prompt, which we will write as $\textbf c$ here.
In addition to the two infix conjunctions \textbf{and} and \textbf{or}, we will also require a suitable bracketing operator, \textbf{conj}, which delimits the predicate being tested.
\begin{align*}
\textbf{conj} &\equiv \angles{\#_\textbf{c}\square} \\
\textbf{and} &\equiv \x,y. \F_\textbf{c}\lambda \_, k.\,(\#_\textbf{c}k\;x)\;\&\&\;(\#_\textbf{c}k\;y) \\
\textbf{or} &\equiv \x,y. \F_\textbf{c}\lambda \_, k.\,(\#_\textbf{c}k\;x)\;||\;(\#_\textbf{c}k\;y) \\
\end{align*}
Of course, additional conjunctions might be implemented; we have limited ourselves to these two for brevity.

Now that we have the definitions, we will give an example reduction sketch.
\begin{align*}
&\textbf{conj}\;(0 == 1\textbf{ or }2\textbf{ or }0) \\
\equiv{}\;\;& \#_\textbf c 0 ==
	(\x,y. \F_\textbf{c}\lambda \_, k.\,(\#_\textbf{c}k\;x)\;||\;(\#_\textbf{c}k\;y))
		\;((\x,y. \F_\textbf{c}\lambda \_, k.\,(\#_\textbf{c}k\;x)\;||\;(\#_\textbf{c}k\;y))
			\;1\;2)
		\;0 \\
%
\mapsto^*{}& \#_\textbf c 0 == \F_\textbf{c}\lambda \_, k.\,(\#_\textbf{c}k\;(\x,y. \F_\textbf{c}\lambda \_, k.\,(\#_\textbf{c}k\;x)\;||\;(\#_\textbf{c}k\;y))
			\;1\;2)\;||\;(\#_\textbf{c}k\;0)\\
%
\mapsto^*{}& (\#_\textbf{c}\angles{0==\square}\;\;(\x,y. \F_\textbf{c}\lambda \_, k.\,(\#_\textbf{c}k\;x)\;||\;(\#_\textbf{c}k\;y))
			\;1\;2)\;||\;(\#_\textbf{c}\angles{0==\square}\;0)\\
%
\mapsto^*{}& (\#_\textbf c 0==\F_\textbf{c}\lambda \_, k.\,(\#_\textbf c k\;1)\;||\;(\#_\textbf c k\;2))
			\;||\;(\#_\textbf c \angles{0==\square}\;0)\\
%
\mapsto^*{}& (\#_\textbf c \angles{0==\square}\;1)\;||\;(\#_\textbf c \angles{0==\square}\;2)\;||\;(\#_\textbf c \angles{0==\square}\;0)\\
\equiv{}\;\;& 0==1\;||\;0==2\;||\;0==0\\
%
\mapsto^*{}& \textbf{true}
\end{align*}

\maybePage
\section{Conclusion}

We have developed a calculus, $\lambda_{SFCC}$ which includes first-class control along with additional operators for managing the scope of external resources.
(TODO FCC is not macro-expressible in the lambda calculus)
We have also been able to show by explicit construction that many control structures, both common and rare, are macro-expressible in $\lambda_{SFCC}$, sometimes with mutable state.
Although mutable state is not macro-expressible in $\lambda_{SFCC}$, this is an orthogonal issue.
In this way, we have shown that the inclusion of a system of FCC like the $\lambda_{SFCC}$ into a language without FCC will strictly increase its expressive power in the sense of \cite{Felleisen90expressive}.

We may ask what the risks are regarding the management of external resources after the inclusion of non-local control, and FCC in particular.
In software engineering, it is widely accepted that any of a few control constructs are sufficient to reduce the risk of resource mis-management to an acceptable level when exceptions are involved.
When FCC is involved, the Scheme community has settled on the dynamic-wind construct for resource management.
Through a comparative analysis, we have identified a symmetry between  these two cases, and have systematically extracted the primitive components of all these techniques.
We have implemented all of these constructs in $\lambda_{SFCC}$.
From the route of its construction, we think it is very likely that our generalization will provide an acceptable level of risk mitigation in the presence of unrestricted FCC.

More subtly, we think the techniques of $\lambda_{SFCC}$ will lead to strictly more reliable systems than are currently in the mainstream.
While resources are handled equally reliably, the strictly more expressive system will allow the abstraction---and thereby encapsulation---of strictly more code patterns.
This abstractive property will reduce the amount of tedious and error-prone boilerplate code.
In particular, we can eliminate boilerplate resource-management code.

Even with these benefits, an observer might still believe that existing languages are ``good enough'' in that they sufficiently cover the range of programs that need to be expressed.
To dispel this idea, we have implemented a wide range of additional control structures not universally present--a few universally not present---using the FCC features of $\lambda_{SFCC}$.
Each of these control structures have practical applications; even during his relatively short programming experience, this author has wished for a few of these structures.
The range we presented was selected in no systematic way: essentially off the top of the author's head.
It is therefore unlikely that the range of applications presented here is in any way comprehensive.
There can be little doubt then that there are applications best expressed with first-class control, and so any language without the feature is not ``good enough''.

A critic might wonder why we have not examined existing programs for the presence of FCC techniques.
If FCC is not commonly used, then why should we increase the complexity of our interpreters and compilers to cope with the additional features?
Such an exercise is analogous to examining Cobol programs for uses of higher-order functions.
The continued success of the Lisp and functional programming communities can only be attributed to the power of higher-order functions, even though Cobol gives no motivation for them.
Current practice is not an indicator of what might be beneficial.
Our argument relies on theoretical utility, with the expectation that what is useful in theory will find a use in practice, as has happened so often in the past.


TODO do we meet the standards from section 5?

TODO macro-expressibility is not really that great, we'd like functional expressiblity.
The trick in many cases is passing arguments unevaluated. If we can do this, there's no problem, and even if we can't, we can protect the argument from evaluation by wrapping it in a thunk before passing.

\subsection{Related Work}

A history of continuations as a means of investigating the formal properties of languages is given in \cite{DiscoverContinuations}.
Since they appear in several areas of research, they were independently discovered on several occasions.

First-class control operators as we might recognize them begin with the \textbf{J} operator in \cite{ISWIM}.
This had the distinction of capturing a continuation which could be used multiple times.
Though it was an ad-hoc introduction meant to model goto statements, it is the ancestor of all of today's FCC operators, including exception handling and call/cc.

After this, variations on FCC operators abounded, especially delimited control operators.
The clearest treatment of first-order delimited control is given in \cite{PromptApplication}, on which we have based much of our formalism.
Higher-order delimited control is examined by \cite{Gunter:1995} in the context of ML; their system allows the introduction of new prompts at runtime, and provides a type system for this technique.
Throw and catch are given in \cite{InterpreterForScheme}, and their generalization to fcontrol in \cite{HandlingControl}.
Since this explosion of operators, work has been done examining the relationships between these operators \cite{ControlDelimitersHierarchy}\cite{Filinski94}\cite{DelimDynBinding}\cite{GreatEscape}.
In this area, \cite{MFDC} is most important for our paper, as it unifies nearly all these control operators short of try-catch and fcontrol.

Work in bringing security to call/cc was done in \cite{continuationsInProcObjs}, which led to the development of dynamic-wind.
A detailed analysis of the shortcomings of exception handling as regards performing cleanup is given in\cite{WeimerNecula08}.
As far as this author can find, we are the first to apply these same security considerations to delimited control.
Recently, work has been done in the efficient implementation of FCC.
Efficient implementations are given for the dynamically-typed Racket is given in \cite{RacketImplFCC}, and for the statically-typed Scala in \cite{ScalaImplFCC}.



\subsection{Further Work}

Although this paper presents a strong theoretical case for the possibility of reliable first-class control, there is still much work left to be done bridging these results into production programming.
In particular, it would be best to gain extensive experience with $\lambda_{SFCC}$ in real programming tasks.
We also expect a serious education campaign would be required to acquaint programmers with these relatively alien techniques.
Beyond these, there are also interesting theoretical questions we did not have time here to treat.

The system we presented here allowed for only one reduction strategy, but we believe this is not an inherent limitation for the system.
Instead, it would be interesting to reformulate our dynamics so that we can take full advantage of compatible closure\cite{LambdaWithTypes}

Since our system was derived from typed systems\cite{MFDC}\cite{Gunter:1995}, we believe it would be possible to assign meaningful types to the terms in $\lambda_{SFCC}$.
The difference for our system is that we allow for a handler to be installed at the delimiting operator.
Whereas in the aforementioned type systems, the type of prompts involved only one type variable, for the type expected by the continuation at the point the delimiter is installed, $\lambda_{SFCC}$ will likely require a second type variable parameterizing the type expected by the handler then installed.

We also find it likely that a type system may need to impose restrictions on the features in our operators.
We mentioned above that the $\#_\blacksquare$ delimiter may be difficult to assign a type, but if there is a single, set handler installed by the runtime system with $\#_\blacksquare$, then the question of its type may be made trivial.
In all likelihood, the operators we have presented are too low-level to be of direct use to users, but will form a framework for implementors to provide slightly higher-level concepts.
Thus, we expect reasonable restrictions on the available features of $\lambda_{SFCC}$ to be prescribed anyway.

Finally, there are clear questions concerning optimizing compilers, given these new techniques.
Some uses of first-class control, e.g. our subroutines in \S\ref{subsec:earlyExit}, are functionally equivalent to a program without FCC, but which instead uses simple jumps.
Further, since subcontinuations are essentially functions, albeit created at runtime, it may be profitable to compile these into real functions at runtime, making use of just0in-time compilation techniques.

















\maybePage
\part{Here There Be Dragons}

NOTE: everything below is in an intermediate state, subject to dramatic change, and probably deletion.



\section{Reformulation}

Key idea is to specify handlers on both prompt and control operators.
We automatically invoke the operator's handler, but control may from there be given to the delimiter's handler.
Essentially, either the control operator has enough information to move the computation along on its own, or else it has enough information to know that it must seek outside help from the delimiter's handler.
This is our answer to choice (4) from the analysis of delimited continuations; its power comes from its ability to model both cases in a unified way.


Exceptions:
\begin{align*}
\textsf{panic }e &\equiv
	\letin{x_{report} = e}{\F_\mathbf{p_0}\lambda \_,\_.x_{report}}
	\\
\end{align*}

Undelimited control:
\begin{align*}
\textsf{withCont}\;e &\equiv
	{}^+\F^-_\mathbf{p_0}e
	\\
\textsf{abort}\;e &\equiv
	\textsf{withCont}\;\lambda \_.e
	\\
\textsf{callcc}\;e &\equiv
	\letin{f=e}{\textsf{withCont}\;\lambda k. f\; \angles{\textsf{abort}\;(k\;\square)}}
	\\
\end{align*}

Misc:
\begin{align*}
\textsf{marker }e &\equiv
	\letin{f=e, x_p=\nu}{\#_{x_p}f\;x_p} \\
\textsf{spawn }e &\equiv
	\letin{f=e}{\textsf{marker }\x_p.f\;\angles{^-\F^+_{x_p}\square}}
	\\
\end{align*}


%If we get something like $\#_\Downarrow^e\F_p^{\F_{p'}\square}v$,
%which leads to $\F_p^{e;\#_\Uparrow^e\F_{p'}\square}v$,
%one might be afraid that that continuation, when called---$\angles{e;\#_\Uparrow^e\F_{p'}\square}\;v'$---will lead to a buildup of $e$ sequenced in a row---perhaps $e;\#_\Uparrow^e\F_{p'}v' \mapsto^* \F_{p'}^{e;(e;\#_\Uparrow^e\square)}v'$.
%This cannot happen, however, since $e$ will need to be forced to a value before the control operator can capture it.
%Preferring to capture continuations wherever possible, the correct evaluation sequence is:
%$$ e;\#_\Uparrow^e\F_{p'}v' \mapsto
%e;\F_{p'}^{e;\#_\Uparrow^e\square}v' \mapsto^*
%v;\F_{p'}^{e;\#_\Uparrow^e\square}v' \mapsto
%\F_{p'}^{e;\#_\Uparrow^e\square}v' $$





\section{Extended Example}
\label{sec:foldl/ee}

TODO file API

TODO pick an example that actually uses capture, though I want to keep some of this analysis

\begin{figure}[H]
\caption{Accumulation-loop pattern codified}
\label{fig:foldl/ee}

%(define (foldl/ee zero xs body)
%   (define mark newCue)
%   (define (break val) (abort c val))
%   (handle mark id
%      (foldl zero xs (body break))))

\begin{subfigure}[b]{0.5\textwidth}
\begin{Verbatim}[commandchars=\\\[\]]
(define (all? xs p)
   (\highlight[foldl/ee] true xs
      (lambda (out acc x)
         (if (p x)
             true
             (out false)))))




\end{Verbatim}
\caption{with FCC}
\end{subfigure}
~
\begin{subfigure}[b]{0.5\textwidth}
\begin{Verbatim}[commandchars=\\\[\]]

define all(xs, p):
    \highlight[var] acc \highlight[=] true
    \highlight[label] out
    \highlight[for] x \highlight[in] xs\highlight[:]
        if (p x):
            \highlight[acc =] true
        else:
            \highlight[acc =] false
            \highlight[break] out
    \highlight[return acc]
\end{Verbatim}
\caption{without FCC}
\end{subfigure}


\end{figure}

Our example is particularly simple, so several simplifying transformations might be automatically made by the programmer.
We have specifically not made these transformations because we would like to examine the accumulation-loop pattern, regardless of the complexity of the problem to which it is applied.

The lines-of-code savings here are not particularly interesting.
For one, this is purely due to surface syntax.
More importantly, as the body becomes larger, the comparison will show less difference in line count.
The important thing to note is that there are fewer ``moving parts'' in the example with FCC -- fewer fixed things that could be misplaced during further maintenance.
Furthermore, even if the test imperative language had foldl already implemented, we cannot re-use it here, so we are forced to repeat -- manually or by copy-paste -- the body of foldl.

note the possibility of nesting calls to foldl/ee.
if Contoy were typed, we show that this is a pure function.

\section{Applications}
\label{application}


\subsection{Software Transactions}

transactions recur, apparently: they're in pep 343 as well as D


\section{Further Directions}

type system: probably using predicative System F$_\omega$. The type of cues would be a two-place type constructor, enforcing purity with monads would rule out control-impure handlers and guards. Cues and subcontunations may need to account for co-/contravariance during type inference.

sandboxing: restrict the environment in which a piece of code executes. as long as there is no way to generate new prompts and you can determine (at least dynamically before you call the untrusted code) which prompts are available to the untrusted code, then you can prevent the untrusted code from gaining control over the rest of the continuation. These are easy things to verify. Of course, the client (source of untrusted code) can still make use of advanced control constructs.

compilation: JIT subcontinuations into functions

Variations:
Should we distinguish a true abort (without possibility of re-instatement) from a capturing abort? If for example we only closed a file on true abort, how would we manage closing the file when the captured continuation is no longer needed?






\maybePage
\bibliographystyle{plain}
\bibliography{fcc}

\end{document}




